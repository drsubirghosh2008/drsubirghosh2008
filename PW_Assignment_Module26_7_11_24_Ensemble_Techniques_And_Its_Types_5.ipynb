{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPrWIBO/Gt/UckgpXrH3zpW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/drsubirghosh2008/drsubirghosh2008/blob/main/PW_Assignment_Module26_7_11_24_Ensemble_Techniques_And_Its_Types_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. You are working on a machine learning project where you have a dataset containing numerical and categorical features. You have identified that some of the features are highly correlated and there are missing values in some of the columns. You want to build a pipeline that automates the feature engineering process and handles the missing values.\n",
        "\n",
        "Design a pipeline that includes the following steps:\n",
        "\n",
        "*Use an automated feature selection method to identify the important features in\n",
        "the dataset\n",
        "*Create a numerical pipeline that includes the following steps:\n",
        "*Impute the missing values in the numerical columns using the mean of the column values\n",
        "*Scale the numerical columns using standardisation\n",
        "*Create a categorical pipeline that includes the following steps:\n",
        "*Impute the missing values in the categorical columns using the most frequent value of the column.\n",
        "*One-hot encode the categorical columns.\n",
        "*Combine the numerical and categorical pipelines using a ColumnTransformer.\n",
        "*Use a Random Forest Classifier to build the final model.\n",
        "*Evaluate the accuracy of the model on the test dataset.\n",
        "\n",
        "Note: Your solution should include code snippets for each step of the pipeline, and a brief explanation of each step. You should also provide an interpretation of the results and suggest possible improvements for the pipeline.\n",
        "\n",
        "Answer:\n",
        "\n",
        "Hereâ€™s a complete pipeline for your machine learning project that addresses feature selection, preprocessing for numerical and categorical columns, and model building with a Random Forest Classifier. This solution also includes code snippets and explanations for each step."
      ],
      "metadata": {
        "id": "LHsSgyOWkJrm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Method-1:"
      ],
      "metadata": {
        "id": "nQv211ZBnCKF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import Required Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.metrics import accuracy_score\n"
      ],
      "metadata": {
        "id": "S2Av5wHlmYuI"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Data and Split into Train and Test Sets\n",
        "# Assuming `df` is your DataFrame, with 'target' as the target variable\n",
        "X = df.drop(columns=['target'])\n",
        "y = df['target']\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "IzW8VotwmySe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Automated Feature Selection\n",
        "# Using a Random Forest to identify important features:\n",
        "\n",
        "# Initialize a Random Forest model for feature selection\n",
        "feature_selector = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "feature_selector.fit(X_train, y_train)\n",
        "\n",
        "# Use SelectFromModel to retain important features\n",
        "sfm = SelectFromModel(feature_selector, threshold=\"mean\")\n",
        "X_train_selected = sfm.transform(X_train)\n",
        "X_test_selected = sfm.transform(X_test)\n",
        "\n",
        "# Get selected feature names for future reference\n",
        "selected_features = X_train.columns[sfm.get_support()]\n"
      ],
      "metadata": {
        "id": "dSVRBLCUmwCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation: This code trains a Random Forest model and uses the SelectFromModel method to select features based on importance. Only features with importance above the mean are retained. The feature selection reduces dimensionality, potentially improving model performance and interpretability."
      ],
      "metadata": {
        "id": "aud7ZDZenL-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Numerical and Categorical Pipelines\n",
        "# a. Numerical Pipeline\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "numerical_pipeline = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')),   # Impute missing values with column mean\n",
        "    ('scaler', StandardScaler())                   # Standardize features\n",
        "])\n"
      ],
      "metadata": {
        "id": "EHvxZhTrnABV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation: The numerical_pipeline imputes missing values in the numerical columns using the column mean and standardizes the data, making each feature have mean zero and unit variance. This scaling is often beneficial for models sensitive to feature magnitudes."
      ],
      "metadata": {
        "id": "n7TS6Ft8nnNd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# b. Categorical Pipeline\n",
        "categorical_pipeline = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')), # Impute missing values with mode\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))    # One-hot encode categorical columns\n",
        "])\n"
      ],
      "metadata": {
        "id": "F1U92MDKnkZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation: The categorical_pipeline fills missing values in the categorical columns with the most frequent value and then one-hot encodes the categorical features. handle_unknown='ignore' prevents issues with new categories in the test set."
      ],
      "metadata": {
        "id": "vjsUfUYGnxLO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine Pipelines with ColumnTransformer\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# Identify the numerical and categorical columns\n",
        "numerical_features = X_train[selected_features].select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = X_train[selected_features].select_dtypes(include=['object', 'category']).columns\n",
        "\n",
        "# Combine pipelines\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_pipeline, numerical_features),\n",
        "        ('cat', categorical_pipeline, categorical_features)\n",
        "    ]\n",
        ")\n"
      ],
      "metadata": {
        "id": "ZYseWOcGnukF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation: ColumnTransformer applies the numerical_pipeline to the numerical features and the categorical_pipeline to the categorical features. This modular design allows separate processing of feature types in one unified pipeline."
      ],
      "metadata": {
        "id": "1gHe5htWn6SF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the Final Pipeline and Train the Model\n",
        "\n",
        "# Create a complete pipeline with preprocessing and model training\n",
        "model_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
        "])\n",
        "\n",
        "# Train the pipeline on selected features\n",
        "model_pipeline.fit(X_train[selected_features], y_train)\n"
      ],
      "metadata": {
        "id": "iEV2ExiEn3wV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the Model on the Test Set\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model_pipeline.predict(X_test[selected_features])\n",
        "\n",
        "# Evaluate model performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "VY-oWVA_oHxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation: We combine preprocessing and model training into a single pipeline. This pipeline is trained on the training data and then used to make predictions on the test set. Finally, the accuracy_score metric evaluates the model's performance on the test data.\n",
        "\n",
        "\n",
        "\n",
        "Interpretation of Results and Potential Improvements\n",
        "Interpretation: The accuracy score reflects the model's performance. If the score is satisfactory, the pipeline can be considered effective for this task. Otherwise, consider further tuning.\n",
        "\n",
        "Possible Improvements:\n",
        "\n",
        "Hyperparameter Tuning: Use GridSearchCV or RandomizedSearchCV to optimize the RandomForestClassifier parameters, such as n_estimators and max_depth.\n",
        "Feature Engineering: Try creating new features or applying domain-specific transformations to improve model performance.\n",
        "Outlier Handling: Consider using robust scaling or removing outliers to reduce noise in the data.\n",
        "Model Selection: Explore other models like Gradient Boosting or XGBoost, which may perform better depending on the data characteristics.\n",
        "This pipeline provides a strong foundation for automating feature engineering, handling missing values, and building a reliable model."
      ],
      "metadata": {
        "id": "8alBnGQIoEkF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Method -2:"
      ],
      "metadata": {
        "id": "rdnUR4sooWYu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To create a robust pipeline that addresses feature selection, missing values, and scaling, hereâ€™s a code example using Python with sklearn. This approach will help automate key steps in the machine learning workflow."
      ],
      "metadata": {
        "id": "0WesTxHjoebl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Sample data\n",
        "# Assuming 'df' is your dataset with features and 'target' is the target column.\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 1. Automated Feature Selection\n",
        "# Use a preliminary RandomForestClassifier to select important features.\n",
        "feature_selector = SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=42))\n",
        "X_train_selected = feature_selector.fit_transform(X_train, y_train)\n",
        "X_test_selected = feature_selector.transform(X_test)\n",
        "\n",
        "# Identify numerical and categorical columns\n",
        "num_features = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
        "cat_features = X_train.select_dtypes(include=['object']).columns\n",
        "\n",
        "# 2. Numerical Pipeline\n",
        "num_pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='mean')),  # Impute missing values with mean\n",
        "    ('scaler', StandardScaler())  # Standardize numerical values\n",
        "])\n",
        "\n",
        "# 3. Categorical Pipeline\n",
        "cat_pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Impute with most frequent value\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))  # One-hot encode categorical variables\n",
        "])\n",
        "\n",
        "# 4. Combine pipelines with ColumnTransformer\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', num_pipeline, num_features),\n",
        "    ('cat', cat_pipeline, cat_features)\n",
        "])\n",
        "\n",
        "# 5. Full Pipeline with RandomForestClassifier\n",
        "pipeline = Pipeline([\n",
        "    ('preprocess', preprocessor),\n",
        "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
        "])\n",
        "\n",
        "# Train the model\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Model accuracy: {accuracy:.2f}')\n"
      ],
      "metadata": {
        "id": "DUYLuIOMof2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "\n",
        "Feature Selection:\n",
        "\n",
        " SelectFromModel uses a preliminary RandomForestClassifier to retain only important features, reducing dimensionality and possibly improving model performance.\n",
        "\n",
        "Numerical Pipeline:\n",
        "\n",
        "This pipeline imputes missing numerical values using the column mean and scales the features to have mean zero and unit variance, enhancing the model's sensitivity to feature magnitudes.\n",
        "\n",
        "Categorical Pipeline:\n",
        "\n",
        "Categorical values are imputed with the most frequent category, and one-hot encoding transforms these columns into a format suitable for machine learning models.\n",
        "\n",
        "ColumnTransformer:\n",
        "\n",
        " Combines numerical and categorical preprocessing pipelines, handling them separately before feeding into the classifier.\n",
        "\n",
        "RandomForestClassifier:\n",
        "\n",
        "The selected model for classification is a robust choice for handling complex datasets and helps in interpreting feature importance.\n",
        "\n",
        "Interpretation of Results and Improvements:\n",
        "\n",
        "The pipeline outputs model accuracy, giving insight into overall performance. To improve accuracy, consider:\n",
        "\n",
        "Hyperparameter Tuning:\n",
        "\n",
        "Use GridSearchCV or RandomizedSearchCV for optimal hyperparameters in RandomForestClassifier.\n",
        "\n",
        "Feature Engineering:\n",
        "\n",
        "Explore polynomial or interaction terms for possible feature enhancement.\n",
        "\n",
        "Outlier Handling:\n",
        "\n",
        "Analyze outliers as they can affect the modelâ€™s accuracy, particularly if they lead to skewed mean imputation.\n",
        "\n",
        "These steps ensure a streamlined, automated workflow that handles missing values, scales features, and performs robust classification."
      ],
      "metadata": {
        "id": "xrP27jfoonbl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. Build a pipeline that includes a random forest classifier and a logistic regression classifier, and then use a voting classifier to combine their predictions. Train the pipeline on the iris dataset and evaluate its accuracy.\n",
        "\n",
        "Answer:\n",
        "\n",
        "To build a pipeline with both a Random Forest and a Logistic Regression classifier, we can use a VotingClassifier to combine their predictions for a final output. Hereâ€™s the implementation for this on the Iris dataset:"
      ],
      "metadata": {
        "id": "utmZSjpQpVNV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Load the iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize individual classifiers\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "lr_clf = LogisticRegression(max_iter=200, random_state=42)\n",
        "\n",
        "# Combine classifiers in a VotingClassifier\n",
        "voting_clf = VotingClassifier(\n",
        "    estimators=[('rf', rf_clf), ('lr', lr_clf)],\n",
        "    voting='soft'  # Soft voting uses the predicted probabilities for averaging\n",
        ")\n",
        "\n",
        "# Create a pipeline with the voting classifier\n",
        "pipeline = Pipeline([\n",
        "    ('voting', voting_clf)\n",
        "])\n",
        "\n",
        "# Train the model\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Voting Classifier accuracy on Iris dataset: {accuracy:.2f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptvMiHsbpiK9",
        "outputId": "c6f4bc2c-fb73-4218-c444-94ae5f1062c8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Voting Classifier accuracy on Iris dataset: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "\n",
        "Load and Split Data: The Iris dataset is loaded, and we split it into training and test sets.\n",
        "\n",
        "Initialize Classifiers:\n",
        "\n",
        "We initialize both the RandomForestClassifier and LogisticRegression. Logistic Regression is set to a higher max_iter to ensure convergence.\n",
        "\n",
        "Voting Classifier:\n",
        "\n",
        "The VotingClassifier combines predictions from both classifiers. Here, we use soft voting, which averages predicted probabilities, often leading to better performance when classifiers output probabilities.\n",
        "\n",
        "Pipeline:\n",
        "\n",
        "A pipeline wraps the voting classifier, making it easy to manage and extend.\n",
        "\n",
        "Train and Evaluate:\n",
        "\n",
        " We train the pipeline and evaluate its accuracy on the test set.\n",
        "\n",
        "Interpretation:\n",
        "\n",
        "The accuracy printed at the end indicates the model's performance on the test set. The VotingClassifier often benefits from combining the strengths of both classifiers (e.g., Random Forest for non-linear relationships and Logistic Regression for interpretability). For further improvement, consider fine-tuning hyperparameters or adding additional classifiers to the ensemble."
      ],
      "metadata": {
        "id": "rBw6i7xips2E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Thank You!**"
      ],
      "metadata": {
        "id": "Y5MNjbp8p-I-"
      }
    }
  ]
}