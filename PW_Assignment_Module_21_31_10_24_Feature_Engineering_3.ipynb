{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPYBSELMvsUYaVGFMI/v7TI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/drsubirghosh2008/drsubirghosh2008/blob/main/PW_Assignment_Module_21_31_10_24_Feature_Engineering_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application.\n",
        "\n",
        "Answer:\n",
        "\n",
        "Min-Max scaling is a normalization technique used in data preprocessing to transform features to a common scale, typically between 0 and 1. This scaling is particularly useful when the features have different units or ranges, as it helps to improve the performance of machine learning algorithms that are sensitive to the scale of the data, such as gradient descent-based methods.\n",
        "\n",
        "How Min-Max Scaling Works\n",
        "The formula for Min-Max scaling is:\n",
        "\n",
        "ğ‘‹\n",
        "â€²\n",
        "=\n",
        "ğ‘‹\n",
        "âˆ’\n",
        "ğ‘‹\n",
        "ğ‘š\n",
        "ğ‘–\n",
        "ğ‘›\n",
        "ğ‘‹\n",
        "ğ‘š\n",
        "ğ‘\n",
        "ğ‘¥\n",
        "âˆ’\n",
        "ğ‘‹\n",
        "ğ‘š\n",
        "ğ‘–\n",
        "ğ‘›\n",
        "X\n",
        "â€²\n",
        " =\n",
        "X\n",
        "max\n",
        "â€‹\n",
        " âˆ’X\n",
        "min\n",
        "â€‹\n",
        "\n",
        "Xâˆ’X\n",
        "min\n",
        "â€‹\n",
        "\n",
        "â€‹\n",
        "\n",
        "Where:\n",
        "\n",
        "ğ‘‹\n",
        "X is the original value.\n",
        "ğ‘‹\n",
        "â€²\n",
        "X\n",
        "â€²\n",
        "  is the scaled value.\n",
        "ğ‘‹\n",
        "ğ‘š\n",
        "ğ‘–\n",
        "ğ‘›\n",
        "X\n",
        "min\n",
        "â€‹\n",
        "  is the minimum value in the feature.\n",
        "ğ‘‹\n",
        "ğ‘š\n",
        "ğ‘\n",
        "ğ‘¥\n",
        "X\n",
        "max\n",
        "â€‹\n",
        "  is the maximum value in the feature.\n",
        "Steps in Min-Max Scaling:\n",
        "Identify the minimum and maximum values of the feature.\n",
        "Apply the Min-Max scaling formula to each value in the feature.\n",
        "Example\n",
        "Consider a dataset with a feature \"Age\" that has the following values:\n",
        "\n",
        "Index\tAge\n",
        "0\t25\n",
        "1\t30\n",
        "2\t45\n",
        "3\t35\n",
        "4\t50\n",
        "Identify Min and Max:\n",
        "\n",
        "ğ‘‹\n",
        "ğ‘š\n",
        "ğ‘–\n",
        "ğ‘›\n",
        "=\n",
        "25\n",
        "X\n",
        "min\n",
        "â€‹\n",
        " =25\n",
        "ğ‘‹\n",
        "ğ‘š\n",
        "ğ‘\n",
        "ğ‘¥\n",
        "=\n",
        "50\n",
        "X\n",
        "max\n",
        "â€‹\n",
        " =50\n",
        "Apply Min-Max Scaling:\n",
        "\n",
        "For Age = 25:\n",
        "ğ‘‹\n",
        "â€²\n",
        "=\n",
        "25\n",
        "âˆ’\n",
        "25\n",
        "50\n",
        "âˆ’\n",
        "25\n",
        "=\n",
        "0\n",
        "25\n",
        "=\n",
        "0\n",
        "X\n",
        "â€²\n",
        " =\n",
        "50âˆ’25\n",
        "25âˆ’25\n",
        "â€‹\n",
        " =\n",
        "25\n",
        "0\n",
        "â€‹\n",
        " =0\n",
        "For Age = 30:\n",
        "ğ‘‹\n",
        "â€²\n",
        "=\n",
        "30\n",
        "âˆ’\n",
        "25\n",
        "50\n",
        "âˆ’\n",
        "25\n",
        "=\n",
        "5\n",
        "25\n",
        "=\n",
        "0.2\n",
        "X\n",
        "â€²\n",
        " =\n",
        "50âˆ’25\n",
        "30âˆ’25\n",
        "â€‹\n",
        " =\n",
        "25\n",
        "5\n",
        "â€‹\n",
        " =0.2\n",
        "For Age = 45:\n",
        "ğ‘‹\n",
        "â€²\n",
        "=\n",
        "45\n",
        "âˆ’\n",
        "25\n",
        "50\n",
        "âˆ’\n",
        "25\n",
        "=\n",
        "20\n",
        "25\n",
        "=\n",
        "0.8\n",
        "X\n",
        "â€²\n",
        " =\n",
        "50âˆ’25\n",
        "45âˆ’25\n",
        "â€‹\n",
        " =\n",
        "25\n",
        "20\n",
        "â€‹\n",
        " =0.8\n",
        "For Age = 35:\n",
        "ğ‘‹\n",
        "â€²\n",
        "=\n",
        "35\n",
        "âˆ’\n",
        "25\n",
        "50\n",
        "âˆ’\n",
        "25\n",
        "=\n",
        "10\n",
        "25\n",
        "=\n",
        "0.4\n",
        "X\n",
        "â€²\n",
        " =\n",
        "50âˆ’25\n",
        "35âˆ’25\n",
        "â€‹\n",
        " =\n",
        "25\n",
        "10\n",
        "â€‹\n",
        " =0.4\n",
        "For Age = 50:\n",
        "ğ‘‹\n",
        "â€²\n",
        "=\n",
        "50\n",
        "âˆ’\n",
        "25\n",
        "50\n",
        "âˆ’\n",
        "25\n",
        "=\n",
        "25\n",
        "25\n",
        "=\n",
        "1\n",
        "X\n",
        "â€²\n",
        " =\n",
        "50âˆ’25\n",
        "50âˆ’25\n",
        "â€‹\n",
        " =\n",
        "25\n",
        "25\n",
        "â€‹\n",
        " =1\n",
        "Resulting Scaled Values:\n",
        "\n",
        "Index\tAge\tScaled Age\n",
        "0\t25\t0\n",
        "1\t30\t0.2\n",
        "2\t45\t0.8\n",
        "3\t35\t0.4\n",
        "4\t50\t1\n",
        "Application\n",
        "Min-Max scaling is commonly used in scenarios such as:\n",
        "\n",
        "When input features have different ranges (e.g., income in thousands and age in years).\n",
        "In neural networks, where activations are sensitive to the scale of inputs.\n",
        "For algorithms like K-means clustering, where distance calculations can be affected by the scale of the features.\n",
        "By applying Min-Max scaling, we can ensure that all features contribute equally to the distance computations and model training processes.\n"
      ],
      "metadata": {
        "id": "g4wKv8vLP-VU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
        "Provide an example to illustrate its application.\n",
        "\n",
        "Answer:\n",
        "\n",
        "\n",
        "The Unit Vector technique, also known as normalization or vector normalization, is a method of feature scaling that transforms feature vectors into unit vectors. This means that the magnitude (or length) of the vector is set to 1, allowing for the comparison of the direction of the vectors, while effectively eliminating the influence of their original scale.\n",
        "\n",
        "How the Unit Vector Technique Works\n",
        "The formula to convert a vector\n",
        "ğ‘‹\n",
        "X into a unit vector\n",
        "ğ‘ˆ\n",
        "U is:\n",
        "\n",
        "ğ‘ˆ\n",
        "=\n",
        "ğ‘‹\n",
        "âˆ¥\n",
        "ğ‘‹\n",
        "âˆ¥\n",
        "U=\n",
        "âˆ¥Xâˆ¥\n",
        "X\n",
        "â€‹\n",
        "\n",
        "Where\n",
        "âˆ¥\n",
        "ğ‘‹\n",
        "âˆ¥\n",
        "âˆ¥Xâˆ¥ is the Euclidean norm (or magnitude) of the vector\n",
        "ğ‘‹\n",
        "X, calculated as:\n",
        "\n",
        "âˆ¥\n",
        "ğ‘‹\n",
        "âˆ¥\n",
        "=\n",
        "âˆ‘\n",
        "ğ‘–\n",
        "=\n",
        "1\n",
        "ğ‘›\n",
        "ğ‘¥\n",
        "ğ‘–\n",
        "2\n",
        "âˆ¥Xâˆ¥=\n",
        "i=1\n",
        "âˆ‘\n",
        "n\n",
        "â€‹\n",
        " x\n",
        "i\n",
        "2\n",
        "â€‹\n",
        "\n",
        "â€‹\n",
        "\n",
        "Key Differences from Min-Max Scaling\n",
        "Range of Values:\n",
        "\n",
        "Min-Max Scaling: Transforms values to a specified range, typically [0, 1].\n",
        "Unit Vector: Normalizes the values so that they have a magnitude of 1, resulting in a unit vector.\n",
        "Geometric Interpretation:\n",
        "\n",
        "Min-Max Scaling: Rescales the data while preserving the original distribution of the data.\n",
        "Unit Vector: Focuses on the direction of the data points in space rather than their actual magnitudes.\n",
        "Use Cases:\n",
        "\n",
        "Min-Max Scaling: Often used when the features have different units or scales.\n",
        "Unit Vector: Useful in scenarios where the direction of the feature matters more than the magnitude, such as in cosine similarity calculations.\n",
        "Example\n",
        "Consider a dataset with a feature vector representing two observations with two features each:\n",
        "\n",
        "Index\tFeature 1\tFeature 2\n",
        "0\t3\t4\n",
        "1\t1\t2\n",
        "Calculate the Euclidean Norm for Each Observation:\n",
        "\n",
        "For the first observation (3, 4):\n",
        "âˆ¥\n",
        "ğ‘‹\n",
        "âˆ¥\n",
        "=\n",
        "3\n",
        "2\n",
        "+\n",
        "4\n",
        "2\n",
        "=\n",
        "9\n",
        "+\n",
        "16\n",
        "=\n",
        "25\n",
        "=\n",
        "5\n",
        "âˆ¥Xâˆ¥=\n",
        "3\n",
        "2\n",
        " +4\n",
        "2\n",
        "\n",
        "â€‹\n",
        " =\n",
        "9+16\n",
        "â€‹\n",
        " =\n",
        "25\n",
        "â€‹\n",
        " =5\n",
        "For the second observation (1, 2):\n",
        "âˆ¥\n",
        "ğ‘‹\n",
        "âˆ¥\n",
        "=\n",
        "1\n",
        "2\n",
        "+\n",
        "2\n",
        "2\n",
        "=\n",
        "1\n",
        "+\n",
        "4\n",
        "=\n",
        "5\n",
        "â‰ˆ\n",
        "2.24\n",
        "âˆ¥Xâˆ¥=\n",
        "1\n",
        "2\n",
        " +2\n",
        "2\n",
        "\n",
        "â€‹\n",
        " =\n",
        "1+4\n",
        "â€‹\n",
        " =\n",
        "5\n",
        "â€‹\n",
        " â‰ˆ2.24\n",
        "Normalize Each Observation to Create Unit Vectors:\n",
        "\n",
        "For the first observation:\n",
        "ğ‘ˆ\n",
        "1\n",
        "=\n",
        "(\n",
        "3\n",
        "5\n",
        ",\n",
        "4\n",
        "5\n",
        ")\n",
        "=\n",
        "(\n",
        "0.6\n",
        ",\n",
        "0.8\n",
        ")\n",
        "U\n",
        "1\n",
        "â€‹\n",
        " =(\n",
        "5\n",
        "3\n",
        "â€‹\n",
        " ,\n",
        "5\n",
        "4\n",
        "â€‹\n",
        " )=(0.6,0.8)\n",
        "For the second observation:\n",
        "ğ‘ˆ\n",
        "2\n",
        "=\n",
        "(\n",
        "1\n",
        "2.24\n",
        ",\n",
        "2\n",
        "2.24\n",
        ")\n",
        "â‰ˆ\n",
        "(\n",
        "0.447\n",
        ",\n",
        "0.894\n",
        ")\n",
        "U\n",
        "2\n",
        "â€‹\n",
        " =(\n",
        "2.24\n",
        "1\n",
        "â€‹\n",
        " ,\n",
        "2.24\n",
        "2\n",
        "â€‹\n",
        " )â‰ˆ(0.447,0.894)\n",
        "Resulting Unit Vectors:\n",
        "\n",
        "Index\tFeature 1\tFeature 2\tUnit Vector (Feature 1, Feature 2)\n",
        "0\t3\t4\t(0.6, 0.8)\n",
        "1\t1\t2\t(0.447, 0.894)\n",
        "Application\n",
        "The Unit Vector technique is particularly useful in scenarios such as:\n",
        "\n",
        "Text Mining: In natural language processing, where documents are represented as vectors and cosine similarity is used for measuring similarity between documents.\n",
        "Clustering Algorithms: Algorithms like K-means may benefit from normalization to unit vectors since they focus on the direction of data points.\n",
        "Distance-based Metrics: When distance measures (like cosine distance) are used, unit vectors help to ensure that the comparisons are solely based on direction rather than magnitude.\n",
        "In summary, the Unit Vector technique focuses on scaling the data to unit length, making it particularly useful when the direction of the data is more important than the actual values."
      ],
      "metadata": {
        "id": "JE1eC-bmQvlK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application.\n",
        "\n",
        "Answer:\n",
        "\n",
        "\n",
        "Principal Component Analysis (PCA) is a statistical technique used for dimensionality reduction while preserving as much variance as possible in a dataset. It transforms the original correlated features into a new set of uncorrelated variables called principal components. These components are ordered by the amount of variance they capture, with the first principal component capturing the most variance.\n",
        "\n",
        "Key Concepts of PCA\n",
        "Variance: PCA seeks to identify directions (principal components) in which the data varies the most.\n",
        "Eigenvalues and Eigenvectors: PCA uses the covariance matrix of the data to find eigenvalues and eigenvectors. The eigenvectors represent the directions of maximum variance, while the eigenvalues indicate the magnitude of variance captured by each eigenvector.\n",
        "Dimensionality Reduction: By selecting a subset of the principal components (usually the top few), PCA reduces the dimensionality of the dataset while maintaining the most important information.\n",
        "Steps Involved in PCA\n",
        "Standardize the Data: Center the data by subtracting the mean and scaling to unit variance.\n",
        "Compute the Covariance Matrix: Calculate the covariance matrix of the standardized data.\n",
        "Compute Eigenvalues and Eigenvectors: Find the eigenvalues and corresponding eigenvectors of the covariance matrix.\n",
        "Sort Eigenvectors: Sort the eigenvectors by decreasing eigenvalues to identify the principal components.\n",
        "Select Top k Components: Choose the top\n",
        "ğ‘˜\n",
        "k eigenvectors to form a new feature space.\n",
        "Transform the Data: Project the original data onto the new feature space defined by the selected principal components.\n",
        "Example of PCA Application\n",
        "Consider a dataset with two features representing the height and weight of a group of individuals:\n",
        "\n",
        "Index\tHeight (cm)\tWeight (kg)\n",
        "0\t150\t50\n",
        "1\t160\t60\n",
        "2\t170\t65\n",
        "3\t180\t80\n",
        "4\t190\t90\n",
        "Step 1: Standardize the Data\n",
        "First, we calculate the mean and standard deviation of each feature and standardize the data:\n",
        "\n",
        "Mean Height = 170 cm, Mean Weight = 67 kg\n",
        "Standard Deviation Height â‰ˆ 14.14 cm, Standard Deviation Weight â‰ˆ 15.81 kg\n",
        "The standardized values are calculated as:\n",
        "\n",
        "ğ‘\n",
        "=\n",
        "ğ‘‹\n",
        "âˆ’\n",
        "Mean\n",
        "StandardÂ Deviation\n",
        "Z=\n",
        "StandardÂ Deviation\n",
        "Xâˆ’Mean\n",
        "â€‹\n",
        "\n",
        "Index\tStandardized Height\tStandardized Weight\n",
        "0\t-1.414\t-1.069\n",
        "1\t-0.707\t-0.447\n",
        "2\t0\t-0.125\n",
        "3\t0.707\t0.822\n",
        "4\t1.414\t1.809\n",
        "Step 2: Compute Covariance Matrix\n",
        "The covariance matrix for the standardized data can be computed. For two features, the covariance matrix will be a 2x2 matrix:\n",
        "\n",
        "Cov\n",
        "=\n",
        "[\n",
        "Var\n",
        "(\n",
        "ğ‘‹\n",
        ")\n",
        "Cov\n",
        "(\n",
        "ğ‘‹\n",
        ",\n",
        "ğ‘Œ\n",
        ")\n",
        "Cov\n",
        "(\n",
        "ğ‘Œ\n",
        ",\n",
        "ğ‘‹\n",
        ")\n",
        "Var\n",
        "(\n",
        "ğ‘Œ\n",
        ")\n",
        "]\n",
        "Cov=[\n",
        "Var(X)\n",
        "Cov(Y,X)\n",
        "â€‹\n",
        "  \n",
        "Cov(X,Y)\n",
        "Var(Y)\n",
        "â€‹\n",
        " ]\n",
        "Step 3: Compute Eigenvalues and Eigenvectors\n",
        "After calculating the covariance matrix, we can compute its eigenvalues and eigenvectors. Suppose we find the following:\n",
        "\n",
        "Eigenvalues:\n",
        "ğœ†\n",
        "1\n",
        "=\n",
        "1.5\n",
        "Î»\n",
        "1\n",
        "â€‹\n",
        " =1.5,\n",
        "ğœ†\n",
        "2\n",
        "=\n",
        "0.5\n",
        "Î»\n",
        "2\n",
        "â€‹\n",
        " =0.5\n",
        "Eigenvectors:\n",
        "ğ‘£\n",
        "1\n",
        "=\n",
        "[\n",
        "0.707\n",
        ",\n",
        "0.707\n",
        "]\n",
        "v\n",
        "1\n",
        "â€‹\n",
        " =[0.707,0.707]\n",
        "ğ‘£\n",
        "2\n",
        "=\n",
        "[\n",
        "âˆ’\n",
        "0.707\n",
        ",\n",
        "0.707\n",
        "]\n",
        "v\n",
        "2\n",
        "â€‹\n",
        " =[âˆ’0.707,0.707]\n",
        "Step 4: Sort Eigenvectors\n",
        "Sort the eigenvectors based on their corresponding eigenvalues in descending order:\n",
        "\n",
        "ğ‘£\n",
        "1\n",
        "v\n",
        "1\n",
        "â€‹\n",
        "  (first principal component)\n",
        "ğ‘£\n",
        "2\n",
        "v\n",
        "2\n",
        "â€‹\n",
        "  (second principal component)\n",
        "Step 5: Select Top k Components\n",
        "Letâ€™s say we want to keep the first principal component (which captures the most variance).\n",
        "\n",
        "Step 6: Transform the Data\n",
        "To transform the original standardized data, we project it onto the first principal component:\n",
        "\n",
        "ğ‘‹\n",
        "â€²\n",
        "=\n",
        "ğ‘‹\n",
        "â‹…\n",
        "ğ‘£\n",
        "1\n",
        "X\n",
        "â€²\n",
        " =Xâ‹…v\n",
        "1\n",
        "â€‹\n",
        "\n",
        "The result will be a new dataset with reduced dimensions, focusing on the most important feature (the direction of maximum variance).\n",
        "\n",
        "Result\n",
        "Index\tPCA Feature (Principal Component)\n",
        "0\t-1.500\n",
        "1\t-0.500\n",
        "2\t0\n",
        "3\t0.500\n",
        "4\t1.500\n",
        "Applications of PCA\n",
        "PCA is widely used in various fields, including:\n",
        "\n",
        "Data Visualization: Reducing high-dimensional data to 2D or 3D for easier visualization.\n",
        "Noise Reduction: Filtering out noise in the data by ignoring lower-variance components.\n",
        "Feature Extraction: Creating new features that summarize the original dataset effectively, which can improve the performance of machine learning algorithms.\n",
        "In summary, PCA is a powerful technique for dimensionality reduction that retains the essential characteristics of the data, facilitating analysis and modeling."
      ],
      "metadata": {
        "id": "MCl09M0ERRjp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept.\n",
        "\n",
        "Answer:\n",
        "\n",
        "\n",
        "Principal Component Analysis (PCA) is closely related to feature extraction, as it provides a method for transforming original features into a new set of features that capture the most variance in the data. The new features (principal components) are uncorrelated and represent linear combinations of the original features, effectively serving as new, informative features that can be used for various data analysis tasks, including machine learning.\n",
        "\n",
        "Relationship between PCA and Feature Extraction\n",
        "Dimensionality Reduction: PCA reduces the number of dimensions in the dataset while preserving as much information (variance) as possible. By selecting only the top principal components, PCA transforms the original feature space into a lower-dimensional space.\n",
        "\n",
        "Feature Representation: The principal components obtained from PCA can be viewed as new features derived from the original features. These components often capture the underlying structure of the data, making them more informative than the original features.\n",
        "\n",
        "Uncorrelated Features: The principal components are orthogonal (uncorrelated) to each other, which helps in eliminating multicollinearity in datasets where features may be correlated. This is beneficial for many machine learning algorithms.\n",
        "\n",
        "How PCA Can Be Used for Feature Extraction\n",
        "PCA can be used for feature extraction in the following steps:\n",
        "\n",
        "Standardize the Data: Center the data by subtracting the mean and scaling it to unit variance.\n",
        "Compute the Covariance Matrix: Calculate the covariance matrix to understand how the features vary together.\n",
        "Calculate Eigenvalues and Eigenvectors: Find the eigenvalues and eigenvectors of the covariance matrix.\n",
        "Sort and Select Principal Components: Sort the eigenvectors based on their corresponding eigenvalues and select the top\n",
        "ğ‘˜\n",
        "k eigenvectors (principal components) that explain the most variance.\n",
        "Transform the Data: Project the original data onto the selected principal components to create a new feature set.\n",
        "Example of PCA for Feature Extraction\n",
        "Consider a dataset of images of handwritten digits (like the MNIST dataset) represented in a high-dimensional space where each image has thousands of pixels (features).\n",
        "\n",
        "Step 1: Standardize the Data\n",
        "Assume we have a small sample of images (flattened to a 3D array for simplicity) with features representing pixel intensities:\n",
        "\n",
        "Image\tPixel 1\tPixel 2\tPixel 3\n",
        "0\t255\t0\t0\n",
        "1\t0\t255\t0\n",
        "2\t0\t0\t255\n",
        "3\t255\t255\t0\n",
        "Calculate mean and standard deviation for each pixel feature.\n",
        "Standardize the pixel values to have a mean of 0 and variance of 1.\n",
        "Step 2: Compute the Covariance Matrix\n",
        "Calculate the covariance matrix of the standardized features.\n",
        "\n",
        "Step 3: Calculate Eigenvalues and Eigenvectors\n",
        "Compute the eigenvalues and eigenvectors of the covariance matrix.\n",
        "\n",
        "Step 4: Sort and Select Principal Components\n",
        "Sort the eigenvalues from highest to lowest and choose the top\n",
        "ğ‘˜\n",
        "k components (e.g., the top 2 components) that explain most of the variance in the data.\n",
        "\n",
        "Step 5: Transform the Data\n",
        "Transform the original dataset into the new feature space defined by the selected principal components.\n",
        "\n",
        "If the eigenvectors corresponding to the top two eigenvalues are:\n",
        "\n",
        "ğ‘£\n",
        "1\n",
        "=\n",
        "[\n",
        "0.707\n",
        ",\n",
        "0.707\n",
        ",\n",
        "0\n",
        "]\n",
        "(\n",
        "ğ‘ƒ\n",
        "ğ¶\n",
        "1\n",
        ")\n",
        "v\n",
        "1\n",
        "â€‹\n",
        " =[0.707,0.707,0](PC1)\n",
        "ğ‘£\n",
        "2\n",
        "=\n",
        "[\n",
        "0.707\n",
        ",\n",
        "âˆ’\n",
        "0.707\n",
        ",\n",
        "0\n",
        "]\n",
        "(\n",
        "ğ‘ƒ\n",
        "ğ¶\n",
        "2\n",
        ")\n",
        "v\n",
        "2\n",
        "â€‹\n",
        " =[0.707,âˆ’0.707,0](PC2)\n",
        "The transformation of each image can be computed as follows:\n",
        "\n",
        "For Image 0:\n",
        "\n",
        "ğ‘ƒ\n",
        "ğ¶\n",
        "1\n",
        "=\n",
        "(\n",
        "255\n",
        "â‹…\n",
        "0.707\n",
        ")\n",
        "+\n",
        "(\n",
        "0\n",
        "â‹…\n",
        "0.707\n",
        ")\n",
        "+\n",
        "(\n",
        "0\n",
        "â‹…\n",
        "0\n",
        ")\n",
        "â‰ˆ\n",
        "180.5\n",
        "PC1=(255â‹…0.707)+(0â‹…0.707)+(0â‹…0)â‰ˆ180.5\n",
        "ğ‘ƒ\n",
        "ğ¶\n",
        "2\n",
        "=\n",
        "(\n",
        "255\n",
        "â‹…\n",
        "0.707\n",
        ")\n",
        "+\n",
        "(\n",
        "0\n",
        "â‹…\n",
        "âˆ’\n",
        "0.707\n",
        ")\n",
        "+\n",
        "(\n",
        "0\n",
        "â‹…\n",
        "0\n",
        ")\n",
        "â‰ˆ\n",
        "180.5\n",
        "PC2=(255â‹…0.707)+(0â‹…âˆ’0.707)+(0â‹…0)â‰ˆ180.5\n",
        "Thus, the new representation for Image 0 in the PCA space could be:\n",
        "\n",
        "Image\tPC1\tPC2\n",
        "0\t180.5\t180.5\n",
        "1\t...\t...\n",
        "2\t...\t...\n",
        "3\t...\t...\n",
        "Applications of PCA for Feature Extraction\n",
        "Image Processing: Reducing dimensionality while retaining essential features, making image classification faster and more efficient.\n",
        "Genomics: Extracting significant patterns in gene expression data.\n",
        "Text Mining: Representing documents in a lower-dimensional space, facilitating text classification or clustering.\n",
        "Conclusion\n",
        "In summary, PCA serves as a powerful tool for feature extraction by transforming high-dimensional data into a lower-dimensional space that retains essential information. The new features (principal components) allow for improved analysis, visualization, and performance of machine learning models.\n"
      ],
      "metadata": {
        "id": "Nhe6oX_QRpuZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data.\n",
        "\n",
        "Answer:\n",
        "\n",
        "To build a recommendation system for a food delivery service, preprocessing the dataset using Min-Max scaling can significantly enhance the model's performance by ensuring that all features contribute equally to the analysis. Given features like price, rating, and delivery time, Min-Max scaling will help normalize these values into a consistent range, typically between 0 and 1. This is especially important in recommendation systems, as it allows the algorithms to treat all features with equal importance when calculating similarities or distances between items.\n",
        "\n",
        "Steps to Apply Min-Max Scaling\n",
        "Hereâ€™s how you can preprocess the data using Min-Max scaling:\n",
        "\n",
        "Step 1: Understand the Dataset\n",
        "Assume the dataset looks like this:\n",
        "\n",
        "Item ID\tPrice ($)\tRating\tDelivery Time (minutes)\n",
        "1\t15\t4.5\t30\n",
        "2\t10\t4.0\t25\n",
        "3\t20\t5.0\t40\n",
        "4\t12\t4.2\t35\n",
        "5\t18\t3.5\t50\n",
        "Step 2: Identify Min and Max Values\n",
        "For each feature, identify the minimum and maximum values:\n",
        "\n",
        "Price:\n",
        "\n",
        "ğ‘‹\n",
        "ğ‘š\n",
        "ğ‘–\n",
        "ğ‘›\n",
        "=\n",
        "10\n",
        "X\n",
        "min\n",
        "â€‹\n",
        " =10\n",
        "ğ‘‹\n",
        "ğ‘š\n",
        "ğ‘\n",
        "ğ‘¥\n",
        "=\n",
        "20\n",
        "X\n",
        "max\n",
        "â€‹\n",
        " =20\n",
        "Rating:\n",
        "\n",
        "ğ‘‹\n",
        "ğ‘š\n",
        "ğ‘–\n",
        "ğ‘›\n",
        "=\n",
        "3.5\n",
        "X\n",
        "min\n",
        "â€‹\n",
        " =3.5\n",
        "ğ‘‹\n",
        "ğ‘š\n",
        "ğ‘\n",
        "ğ‘¥\n",
        "=\n",
        "5.0\n",
        "X\n",
        "max\n",
        "â€‹\n",
        " =5.0\n",
        "Delivery Time:\n",
        "\n",
        "ğ‘‹\n",
        "ğ‘š\n",
        "ğ‘–\n",
        "ğ‘›\n",
        "=\n",
        "25\n",
        "X\n",
        "min\n",
        "â€‹\n",
        " =25\n",
        "ğ‘‹\n",
        "ğ‘š\n",
        "ğ‘\n",
        "ğ‘¥\n",
        "=\n",
        "50\n",
        "X\n",
        "max\n",
        "â€‹\n",
        " =50\n",
        "Step 3: Apply Min-Max Scaling\n",
        "Use the Min-Max scaling formula for each feature:\n",
        "\n",
        "ğ‘‹\n",
        "â€²\n",
        "=\n",
        "ğ‘‹\n",
        "âˆ’\n",
        "ğ‘‹\n",
        "ğ‘š\n",
        "ğ‘–\n",
        "ğ‘›\n",
        "ğ‘‹\n",
        "ğ‘š\n",
        "ğ‘\n",
        "ğ‘¥\n",
        "âˆ’\n",
        "ğ‘‹\n",
        "ğ‘š\n",
        "ğ‘–\n",
        "ğ‘›\n",
        "X\n",
        "â€²\n",
        " =\n",
        "X\n",
        "max\n",
        "â€‹\n",
        " âˆ’X\n",
        "min\n",
        "â€‹\n",
        "\n",
        "Xâˆ’X\n",
        "min\n",
        "â€‹\n",
        "\n",
        "â€‹\n",
        "\n",
        "Calculate the scaled values for each feature.\n",
        "\n",
        "Scaled Price:\n",
        "\n",
        "For Item 1:\n",
        "Price\n",
        "1\n",
        "â€²\n",
        "=\n",
        "15\n",
        "âˆ’\n",
        "10\n",
        "20\n",
        "âˆ’\n",
        "10\n",
        "=\n",
        "5\n",
        "10\n",
        "=\n",
        "0.5\n",
        "Price\n",
        "1\n",
        "â€²\n",
        "â€‹\n",
        " =\n",
        "20âˆ’10\n",
        "15âˆ’10\n",
        "â€‹\n",
        " =\n",
        "10\n",
        "5\n",
        "â€‹\n",
        " =0.5\n",
        "For Item 2:\n",
        "Price\n",
        "2\n",
        "â€²\n",
        "=\n",
        "10\n",
        "âˆ’\n",
        "10\n",
        "20\n",
        "âˆ’\n",
        "10\n",
        "=\n",
        "0\n",
        "Price\n",
        "2\n",
        "â€²\n",
        "â€‹\n",
        " =\n",
        "20âˆ’10\n",
        "10âˆ’10\n",
        "â€‹\n",
        " =0\n",
        "For Item 3:\n",
        "Price\n",
        "3\n",
        "â€²\n",
        "=\n",
        "20\n",
        "âˆ’\n",
        "10\n",
        "20\n",
        "âˆ’\n",
        "10\n",
        "=\n",
        "1\n",
        "Price\n",
        "3\n",
        "â€²\n",
        "â€‹\n",
        " =\n",
        "20âˆ’10\n",
        "20âˆ’10\n",
        "â€‹\n",
        " =1\n",
        "For Item 4:\n",
        "Price\n",
        "4\n",
        "â€²\n",
        "=\n",
        "12\n",
        "âˆ’\n",
        "10\n",
        "20\n",
        "âˆ’\n",
        "10\n",
        "=\n",
        "2\n",
        "10\n",
        "=\n",
        "0.2\n",
        "Price\n",
        "4\n",
        "â€²\n",
        "â€‹\n",
        " =\n",
        "20âˆ’10\n",
        "12âˆ’10\n",
        "â€‹\n",
        " =\n",
        "10\n",
        "2\n",
        "â€‹\n",
        " =0.2\n",
        "For Item 5:\n",
        "Price\n",
        "5\n",
        "â€²\n",
        "=\n",
        "18\n",
        "âˆ’\n",
        "10\n",
        "20\n",
        "âˆ’\n",
        "10\n",
        "=\n",
        "8\n",
        "10\n",
        "=\n",
        "0.8\n",
        "Price\n",
        "5\n",
        "â€²\n",
        "â€‹\n",
        " =\n",
        "20âˆ’10\n",
        "18âˆ’10\n",
        "â€‹\n",
        " =\n",
        "10\n",
        "8\n",
        "â€‹\n",
        " =0.8\n",
        "Scaled Rating:\n",
        "\n",
        "For Item 1:\n",
        "Rating\n",
        "1\n",
        "â€²\n",
        "=\n",
        "4.5\n",
        "âˆ’\n",
        "3.5\n",
        "5.0\n",
        "âˆ’\n",
        "3.5\n",
        "=\n",
        "1.0\n",
        "1.5\n",
        "â‰ˆ\n",
        "0.67\n",
        "Rating\n",
        "1\n",
        "â€²\n",
        "â€‹\n",
        " =\n",
        "5.0âˆ’3.5\n",
        "4.5âˆ’3.5\n",
        "â€‹\n",
        " =\n",
        "1.5\n",
        "1.0\n",
        "â€‹\n",
        " â‰ˆ0.67\n",
        "For Item 2:\n",
        "Rating\n",
        "2\n",
        "â€²\n",
        "=\n",
        "4.0\n",
        "âˆ’\n",
        "3.5\n",
        "5.0\n",
        "âˆ’\n",
        "3.5\n",
        "=\n",
        "0.5\n",
        "1.5\n",
        "â‰ˆ\n",
        "0.33\n",
        "Rating\n",
        "2\n",
        "â€²\n",
        "â€‹\n",
        " =\n",
        "5.0âˆ’3.5\n",
        "4.0âˆ’3.5\n",
        "â€‹\n",
        " =\n",
        "1.5\n",
        "0.5\n",
        "â€‹\n",
        " â‰ˆ0.33\n",
        "For Item 3:\n",
        "Rating\n",
        "3\n",
        "â€²\n",
        "=\n",
        "5.0\n",
        "âˆ’\n",
        "3.5\n",
        "5.0\n",
        "âˆ’\n",
        "3.5\n",
        "=\n",
        "1.5\n",
        "1.5\n",
        "=\n",
        "1\n",
        "Rating\n",
        "3\n",
        "â€²\n",
        "â€‹\n",
        " =\n",
        "5.0âˆ’3.5\n",
        "5.0âˆ’3.5\n",
        "â€‹\n",
        " =\n",
        "1.5\n",
        "1.5\n",
        "â€‹\n",
        " =1\n",
        "For Item 4:\n",
        "Rating\n",
        "4\n",
        "â€²\n",
        "=\n",
        "4.2\n",
        "âˆ’\n",
        "3.5\n",
        "5.0\n",
        "âˆ’\n",
        "3.5\n",
        "=\n",
        "0.7\n",
        "1.5\n",
        "â‰ˆ\n",
        "0.47\n",
        "Rating\n",
        "4\n",
        "â€²\n",
        "â€‹\n",
        " =\n",
        "5.0âˆ’3.5\n",
        "4.2âˆ’3.5\n",
        "â€‹\n",
        " =\n",
        "1.5\n",
        "0.7\n",
        "â€‹\n",
        " â‰ˆ0.47\n",
        "For Item 5:\n",
        "Rating\n",
        "5\n",
        "â€²\n",
        "=\n",
        "3.5\n",
        "âˆ’\n",
        "3.5\n",
        "5.0\n",
        "âˆ’\n",
        "3.5\n",
        "=\n",
        "0\n",
        "Rating\n",
        "5\n",
        "â€²\n",
        "â€‹\n",
        " =\n",
        "5.0âˆ’3.5\n",
        "3.5âˆ’3.5\n",
        "â€‹\n",
        " =0\n",
        "Scaled Delivery Time:\n",
        "\n",
        "For Item 1:\n",
        "DeliveryÂ Time\n",
        "1\n",
        "â€²\n",
        "=\n",
        "30\n",
        "âˆ’\n",
        "25\n",
        "50\n",
        "âˆ’\n",
        "25\n",
        "=\n",
        "5\n",
        "25\n",
        "=\n",
        "0.2\n",
        "DeliveryÂ Time\n",
        "1\n",
        "â€²\n",
        "â€‹\n",
        " =\n",
        "50âˆ’25\n",
        "30âˆ’25\n",
        "â€‹\n",
        " =\n",
        "25\n",
        "5\n",
        "â€‹\n",
        " =0.2\n",
        "For Item 2:\n",
        "DeliveryÂ Time\n",
        "2\n",
        "â€²\n",
        "=\n",
        "25\n",
        "âˆ’\n",
        "25\n",
        "50\n",
        "âˆ’\n",
        "25\n",
        "=\n",
        "0\n",
        "DeliveryÂ Time\n",
        "2\n",
        "â€²\n",
        "â€‹\n",
        " =\n",
        "50âˆ’25\n",
        "25âˆ’25\n",
        "â€‹\n",
        " =0\n",
        "For Item 3:\n",
        "DeliveryÂ Time\n",
        "3\n",
        "â€²\n",
        "=\n",
        "40\n",
        "âˆ’\n",
        "25\n",
        "50\n",
        "âˆ’\n",
        "25\n",
        "=\n",
        "15\n",
        "25\n",
        "=\n",
        "0.6\n",
        "DeliveryÂ Time\n",
        "3\n",
        "â€²\n",
        "â€‹\n",
        " =\n",
        "50âˆ’25\n",
        "40âˆ’25\n",
        "â€‹\n",
        " =\n",
        "25\n",
        "15\n",
        "â€‹\n",
        " =0.6\n",
        "For Item 4:\n",
        "DeliveryÂ Time\n",
        "4\n",
        "â€²\n",
        "=\n",
        "35\n",
        "âˆ’\n",
        "25\n",
        "50\n",
        "âˆ’\n",
        "25\n",
        "=\n",
        "10\n",
        "25\n",
        "=\n",
        "0.4\n",
        "DeliveryÂ Time\n",
        "4\n",
        "â€²\n",
        "â€‹\n",
        " =\n",
        "50âˆ’25\n",
        "35âˆ’25\n",
        "â€‹\n",
        " =\n",
        "25\n",
        "10\n",
        "â€‹\n",
        " =0.4\n",
        "For Item 5:\n",
        "DeliveryÂ Time\n",
        "5\n",
        "â€²\n",
        "=\n",
        "50\n",
        "âˆ’\n",
        "25\n",
        "50\n",
        "âˆ’\n",
        "25\n",
        "=\n",
        "1\n",
        "DeliveryÂ Time\n",
        "5\n",
        "â€²\n",
        "â€‹\n",
        " =\n",
        "50âˆ’25\n",
        "50âˆ’25\n",
        "â€‹\n",
        " =1\n",
        "Step 4: Resulting Scaled Dataset\n",
        "The scaled dataset would look like this:\n",
        "\n",
        "Item ID\tScaled Price\tScaled Rating\tScaled Delivery Time\n",
        "1\t0.5\t0.67\t0.2\n",
        "2\t0\t0.33\t0\n",
        "3\t1\t1\t0.6\n",
        "4\t0.2\t0.47\t0.4\n",
        "5\t0.8\t0\t1\n",
        "Benefits of Using Min-Max Scaling in Recommendation Systems\n",
        "Equal Importance: Ensures that features like price, rating, and delivery time contribute equally to distance calculations, which is crucial for algorithms that rely on these metrics (e.g., collaborative filtering).\n",
        "Improved Convergence: Helps gradient-based algorithms (like neural networks) converge faster by providing a consistent scale.\n",
        "Distance Metrics: Facilitates better performance of distance-based metrics (e.g., cosine similarity, Euclidean distance) used in the recommendation system.\n",
        "Conclusion\n",
        "Using Min-Max scaling as a preprocessing step in your food delivery recommendation system can enhance the performance of the model by standardizing the feature values, ensuring that all features contribute appropriately to the calculations involved in generating recommendations. This normalization process is essential in ensuring that the recommendation system functions effectively and provides accurate suggestions to users."
      ],
      "metadata": {
        "id": "CAx9FA3KR3o7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset.\n",
        "\n",
        "Answer:\n",
        "\n",
        "When building a model to predict stock prices, especially with a dataset that includes a variety of features such as company financial data (e.g., revenue, profit margins, debt levels) and market trends (e.g., trading volume, historical prices), Principal Component Analysis (PCA) can be a valuable tool for reducing dimensionality. By using PCA, you can simplify your model, mitigate overfitting, and potentially improve predictive performance.\n",
        "\n",
        "Steps to Use PCA for Dimensionality Reduction in Stock Price Prediction\n",
        "Step 1: Data Preparation\n",
        "Collect and Clean Data: Ensure that your dataset is clean and well-structured, with relevant features for stock price prediction. Handle any missing values, outliers, or inconsistencies.\n",
        "\n",
        "Feature Selection: Identify which features are essential for your analysis. This could include financial ratios, historical stock prices, and market indicators.\n",
        "\n",
        "Step 2: Standardize the Data\n",
        "PCA is sensitive to the scales of the features, so it's important to standardize the dataset to ensure each feature contributes equally to the analysis. You can standardize the data using the z-score normalization:\n",
        "\n",
        "ğ‘\n",
        "=\n",
        "ğ‘‹\n",
        "âˆ’\n",
        "mean\n",
        "standardÂ deviation\n",
        "Z=\n",
        "standardÂ deviation\n",
        "Xâˆ’mean\n",
        "â€‹\n",
        "\n",
        "This process ensures that the features have a mean of 0 and a standard deviation of 1.\n",
        "\n",
        "Step 3: Compute the Covariance Matrix\n",
        "After standardizing the dataset, calculate the covariance matrix. The covariance matrix provides insight into how the features relate to one another and will help identify the directions of maximum variance.\n",
        "\n",
        "Step 4: Calculate Eigenvalues and Eigenvectors\n",
        "Determine the eigenvalues and eigenvectors of the covariance matrix. The eigenvalues indicate the amount of variance captured by each principal component, while the eigenvectors provide the directions of the new feature space.\n",
        "\n",
        "Step 5: Sort Eigenvalues and Select Principal Components\n",
        "Sort the eigenvalues in descending order. Choose the top\n",
        "ğ‘˜\n",
        "k principal components that explain the majority of the variance in the data. This is typically done by examining the explained variance ratio, which shows how much variance is captured by each principal component.\n",
        "\n",
        "For example, you might decide to retain enough components to explain 95% of the variance.\n",
        "\n",
        "Step 6: Transform the Data\n",
        "Project the original standardized data onto the selected principal components to create a new reduced-dimensional dataset. This can be done by multiplying the standardized dataset by the matrix of selected eigenvectors:\n",
        "\n",
        "ğ‘‹\n",
        "â€²\n",
        "=\n",
        "ğ‘‹\n",
        "â‹…\n",
        "ğ‘‰\n",
        "ğ‘˜\n",
        "X\n",
        "â€²\n",
        " =Xâ‹…V\n",
        "k\n",
        "â€‹\n",
        "\n",
        "Where\n",
        "ğ‘‰\n",
        "ğ‘˜\n",
        "V\n",
        "k\n",
        "â€‹\n",
        "  is the matrix of selected eigenvectors.\n",
        "\n",
        "Step 7: Use the Reduced Dataset for Modeling\n",
        "The resulting dataset, with reduced dimensions, can now be used as input features for your stock price prediction model. This model could be a regression algorithm (e.g., linear regression, random forest regression) or a more complex algorithm (e.g., neural networks).\n",
        "\n",
        "Example Workflow\n",
        "Original Features:\n",
        "\n",
        "Financial Data: Revenue, Earnings Per Share (EPS), Debt-to-Equity Ratio\n",
        "Market Trends: Moving Averages, Trading Volume, Volatility Measures\n",
        "Standardize the Data:\n",
        "\n",
        "Standardized revenue: Z-score normalization.\n",
        "Standardized EPS: Z-score normalization.\n",
        "Covariance Matrix:\n",
        "\n",
        "Calculate the covariance matrix for standardized features.\n",
        "Eigenvalues and Eigenvectors:\n",
        "\n",
        "Compute eigenvalues and eigenvectors of the covariance matrix.\n",
        "Sort and Select:\n",
        "\n",
        "Assume the first three eigenvalues capture 95% of the variance; select the corresponding eigenvectors.\n",
        "Transform the Data:\n",
        "\n",
        "Project the standardized data onto the new feature space defined by the selected principal components.\n",
        "Model Training:\n",
        "\n",
        "Train your prediction model (e.g., using the new features to predict future stock prices).\n",
        "Benefits of Using PCA in Stock Price Prediction\n",
        "Dimensionality Reduction: Reduces the number of features, simplifying the model and making it easier to interpret.\n",
        "Mitigation of Overfitting: By reducing the complexity of the dataset, PCA helps mitigate overfitting, especially when dealing with high-dimensional data.\n",
        "Noise Reduction: PCA can help filter out noise in the data by focusing on the components that capture the most variance, which can improve the performance of your model.\n",
        "Improved Computational Efficiency: With fewer dimensions, training and prediction times can be reduced, which is beneficial for models that require extensive computation.\n",
        "Conclusion\n",
        "In summary, using PCA to reduce the dimensionality of a stock price prediction dataset can enhance the model's performance by simplifying the feature space while preserving the essential information."
      ],
      "metadata": {
        "id": "6gXTm6T1SB5C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1.\n",
        "\n",
        "Answer:\n",
        "\n",
        "The Min-Max scaling of the dataset\n",
        "[\n",
        "1\n",
        ",\n",
        "5\n",
        ",\n",
        "10\n",
        ",\n",
        "15\n",
        ",\n",
        "20\n",
        "]\n",
        "[1,5,10,15,20] transformed to a range of\n",
        "[\n",
        "âˆ’\n",
        "1\n",
        ",\n",
        "1\n",
        "]\n",
        "[âˆ’1,1] results in the following values:\n",
        "\n",
        "[\n",
        "âˆ’\n",
        "1.0\n",
        ",\n",
        "âˆ’\n",
        "0.57894737\n",
        ",\n",
        "âˆ’\n",
        "0.05263158\n",
        ",\n",
        "0.47368421\n",
        ",\n",
        "1.0\n",
        "]\n",
        "[âˆ’1.0,âˆ’0.57894737,âˆ’0.05263158,0.47368421,1.0]\n",
        "Explanation of the Transformation:\n",
        "Original Values: 1, 5, 10, 15, 20\n",
        "Minimum Value: 1\n",
        "Maximum Value: 20\n",
        "The Min-Max scaling formula used here is:\n",
        "\n",
        "ğ‘‹\n",
        "â€²\n",
        "=\n",
        "2\n",
        "Ã—\n",
        "(\n",
        "ğ‘‹\n",
        "âˆ’\n",
        "ğ‘‹\n",
        "min\n",
        "ğ‘‹\n",
        "max\n",
        "âˆ’\n",
        "ğ‘‹\n",
        "min\n",
        ")\n",
        "âˆ’\n",
        "1\n",
        "X\n",
        "â€²\n",
        " =2Ã—(\n",
        "X\n",
        "max\n",
        "â€‹\n",
        " âˆ’X\n",
        "min\n",
        "â€‹\n",
        "\n",
        "Xâˆ’X\n",
        "min\n",
        "â€‹\n",
        "\n",
        "â€‹\n",
        " )âˆ’1\n",
        "Where:\n",
        "\n",
        "ğ‘‹\n",
        "X is the original value\n",
        "ğ‘‹\n",
        "min\n",
        "X\n",
        "min\n",
        "â€‹\n",
        "  is the minimum value in the dataset\n",
        "ğ‘‹\n",
        "max\n",
        "X\n",
        "max\n",
        "â€‹\n",
        "  is the maximum value in the dataset\n",
        "This scaling effectively maps the original data points to a new range of\n",
        "[\n",
        "âˆ’\n",
        "1\n",
        ",\n",
        "1\n",
        "]\n",
        "[âˆ’1,1]"
      ],
      "metadata": {
        "id": "4vMg2NbNSZj7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Original dataset\n",
        "data = np.array([1, 5, 10, 15, 20])\n",
        "\n",
        "# Min-Max Scaling to range [-1, 1]\n",
        "data_min = data.min()\n",
        "data_max = data.max()\n",
        "\n",
        "# Scaling formula: X' = 2 * ((X - X_min) / (X_max - X_min)) - 1\n",
        "scaled_data = 2 * (data - data_min) / (data_max - data_min) - 1\n",
        "scaled_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7Lr4JGqTn0s",
        "outputId": "8de9cfd4-503e-404b-b8db-c569913bd46f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-1.        , -0.57894737, -0.05263158,  0.47368421,  1.        ])"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Performing feature extraction using Principal Component Analysis (PCA) involves several steps, including standardizing the data, calculating the covariance matrix, and determining the eigenvalues and eigenvectors. While I canâ€™t compute PCA without specific numerical values for your dataset, I can guide you through the process and explain how to decide how many principal components to retain.\n",
        "\n",
        "Steps for PCA\n",
        "Data Collection: Assume you have a dataset with numerical features, such as height, weight, age, and blood pressure, along with a categorical feature (gender) which may need to be encoded before applying PCA.\n",
        "\n",
        "Standardization: Since PCA is sensitive to the variances of the features, standardize the numerical features to have a mean of 0 and a standard deviation of 1.\n",
        "\n",
        "Covariance Matrix Calculation: Calculate the covariance matrix of the standardized features. This matrix expresses how much the dimensions vary from the mean with respect to each other.\n",
        "\n",
        "Eigenvalues and Eigenvectors: Compute the eigenvalues and eigenvectors of the covariance matrix. The eigenvalues indicate the amount of variance captured by each principal component.\n",
        "\n",
        "Explained Variance Ratio: Calculate the explained variance ratio for each principal component. This helps in determining how much information (variance) is retained by each component.\n",
        "\n",
        "How Many Principal Components to Retain\n",
        "When deciding how many principal components to retain, consider the following approaches:\n",
        "\n",
        "Cumulative Explained Variance:\n",
        "\n",
        "Plot the cumulative explained variance against the number of principal components. The goal is to choose the smallest number of components that explain a sufficiently large percentage of the total variance (e.g., 90-95%).\n",
        "For example, if the first 3 principal components explain 92% of the variance, you may choose to retain those three.\n",
        "Elbow Method:\n",
        "\n",
        "Create a scree plot of the eigenvalues. Look for a point where the eigenvalues start to level off (the \"elbow\"). The number of components before this point is often a good choice.\n",
        "Domain Knowledge:\n",
        "\n",
        "Consider the specific context of your analysis. For instance, if height and weight are highly correlated, you might find that a single principal component can adequately represent both.\n",
        "Example Decision\n",
        "Let's assume after performing PCA on the features [height, weight, age, blood pressure], you compute the following explained variance ratios:\n",
        "\n",
        "PC1: 50%\n",
        "PC2: 30%\n",
        "PC3: 15%\n",
        "PC4: 5%\n",
        "To determine how many principal components to retain:\n",
        "\n",
        "Cumulative Explained Variance:\n",
        "PC1 + PC2: 80%\n",
        "PC1 + PC2 + PC3: 95%\n",
        "Based on this, you might decide to retain 3 principal components because together they explain 95% of the variance.\n",
        "\n",
        "Conclusion\n",
        "In summary, the decision on how many principal components to retain should be based on the cumulative explained variance and the specific goals of the analysis."
      ],
      "metadata": {
        "id": "EabDKBZvTEeK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Thank You!**"
      ],
      "metadata": {
        "id": "b4N6XibuUELr"
      }
    }
  ]
}