{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMOambY68dZCLvLyLV1Gqxj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/drsubirghosh2008/drsubirghosh2008/blob/main/PW_Assignment_Module_26_05_11_24_Na%C3%AFve_bayes_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. A company conducted a survey of its employees and found that 70% of the employees use the company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the probability that an employee is a smoker given that he/she uses the health insurance plan?\n",
        "\n",
        "Answer:\n",
        "\n",
        "We can solve this problem using conditional probability. Let:\n",
        "\n",
        "ùê¥\n",
        "A be the event that an employee uses the company's health insurance plan.\n",
        "ùêµ\n",
        "B be the event that an employee is a smoker.\n",
        "We are given:\n",
        "\n",
        "ùëÉ\n",
        "(\n",
        "ùê¥\n",
        ")\n",
        "=\n",
        "0.7\n",
        "P(A)=0.7, the probability that an employee uses the health insurance plan.\n",
        "ùëÉ\n",
        "(\n",
        "ùêµ\n",
        "‚à£\n",
        "ùê¥\n",
        ")\n",
        "=\n",
        "0.4\n",
        "P(B‚à£A)=0.4, the probability that an employee is a smoker given that they use the health insurance plan.\n",
        "The probability that an employee is a smoker given that they use the health insurance plan is simply\n",
        "ùëÉ\n",
        "(\n",
        "ùêµ\n",
        "‚à£\n",
        "ùê¥\n",
        ")\n",
        "P(B‚à£A), which we are given as 0.4 (or 40%).\n",
        "\n",
        "So, the probability that an employee is a smoker given that they use the health insurance plan is:\n",
        "\n",
        "0.4\n",
        "¬†or\n",
        "40\n",
        "%\n",
        "0.4¬†or¬†40%\n",
        "‚Äã\n"
      ],
      "metadata": {
        "id": "8EQq3a-XUDfs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?\n",
        "\n",
        "Answer:\n",
        "\n",
        "The main difference between Bernoulli Naive Bayes and Multinomial Naive Bayes lies in how they handle different types of data, particularly in text classification.\n",
        "\n",
        "1. Bernoulli Naive Bayes\n",
        "Data Assumption: Bernoulli Naive Bayes works with binary data (0s and 1s). It assumes that features are binary indicators representing the presence (1) or absence (0) of a feature.\n",
        "\n",
        "Application: Commonly used in binary feature spaces, such as in text classification where we are only interested in whether a particular word occurs in a document, not how many times it occurs.\n",
        "\n",
        "Example: In text classification, a document is represented by whether or not each word in a predefined vocabulary appears in it. For instance, if the vocabulary is \"good,\" \"bad,\" \"happy,\" and \"sad,\" a document could be represented as [1, 0, 1, 0] if \"good\" and \"happy\" are present, but \"bad\" and \"sad\" are not.\n",
        "\n",
        "2. Multinomial Naive Bayes\n",
        "Data Assumption: Multinomial Naive Bayes works with count data, assuming that features are counts or frequencies of events. It expects integer values representing the number of times each feature appears.\n",
        "Application: Commonly used for text classification where we consider not only the presence of words but also their frequency in a document.\n",
        "\n",
        "Example: In text classification, a document is represented by the counts of each word in a predefined vocabulary. If the vocabulary is \"good,\" \"bad,\" \"happy,\" and \"sad,\" a document could be represented as [2, 0, 1, 0] if \"good\" appears twice, \"bad\" is absent, \"happy\" appears once, and \"sad\" is absent.\n",
        "\n",
        "Summary of Key Differences:\n",
        "\n",
        "Aspect\tBernoulli Naive Bayes\tMultinomial Naive Bayes\n",
        "Data Type\tBinary data (0 or 1)\tCount data (integer frequency values)\n",
        "Feature Representation\tPresence/absence of features\tFrequency of features\n",
        "Typical Use Case\tBinary text classification (spam detection)\tFrequency-based text classification (sentiment analysis)\n",
        "\n",
        "In short:\n",
        "\n",
        "Bernoulli Naive Bayes is suited for binary features (presence or absence).\n",
        "Multinomial Naive Bayes is suited for features with count or frequency values."
      ],
      "metadata": {
        "id": "SdL220p8UzxV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. How does Bernoulli Naive Bayes handle missing values?\n",
        "\n",
        "Answer:\n",
        "\n",
        "In Bernoulli Naive Bayes, missing values are typically treated as absent features, meaning they are assigned a value of 0, indicating that the feature does not appear. This is because Bernoulli Naive Bayes is designed to work with binary features where each feature's presence or absence is significant.\n",
        "\n",
        "Handling Missing Values in Bernoulli Naive Bayes:\n",
        "Assume Absence (0): When a feature's value is missing for a particular instance, it is generally treated as 0 (absent) by default. For example, if we're classifying text documents and a word is missing in a document, Bernoulli Naive Bayes interprets this as the word simply not appearing in the document.\n",
        "\n",
        "Imputation Techniques (If Needed): If treating missing values as \"absent\" isn‚Äôt appropriate in a specific context, other imputation techniques can be used before fitting the model. For instance, missing values can be imputed based on the mean, mode, or other methods depending on the nature of the dataset.\n",
        "\n",
        "In practice, treating missing values as 0 often aligns with the model's binary assumption and does not significantly affect performance in typical text-based applications."
      ],
      "metadata": {
        "id": "bTOw-emQV0Al"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. Can Gaussian Naive Bayes be used for multi-class classification?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Yes, Gaussian Naive Bayes can be used for multi-class classification. Although it is often associated with binary classification problems, Gaussian Naive Bayes is inherently capable of handling multiple classes due to the way it calculates probabilities for each class separately.\n",
        "\n",
        "How Gaussian Naive Bayes Works for Multi-Class Classification?\n",
        "\n",
        "In Gaussian Naive Bayes, we assume that each feature follows a Gaussian (normal) distribution within each class. For multi-class classification:\n",
        "\n",
        "Separate Class Distributions: Gaussian Naive Bayes calculates the probability density function of each feature for every class based on the mean and variance of that feature within each class.\n",
        "\n",
        "Class Conditional Probability: During prediction, the model calculates the probability of the data belonging to each class (based on Bayes' theorem) and assigns the instance to the class with the highest probability.\n",
        "\n",
        "Example\n",
        "For example, if you have three classes (Class A, Class B, Class C), Gaussian Naive Bayes will:\n",
        "\n",
        "Estimate the mean and variance of each feature separately for Class A, Class B, and Class C.\n",
        "\n",
        "During prediction, compute the likelihood of a data point for each class.\n",
        "Choose the class with the highest posterior probability as the predicted class.\n",
        "\n",
        "Summary\n",
        "\n",
        "Gaussian Naive Bayes is well-suited for multi-class classification and can handle any number of classes by estimating the necessary parameters (mean and variance) for each feature within each class.\n"
      ],
      "metadata": {
        "id": "DEPm9nJBViDs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. Assignment:\n",
        "\n",
        "Data preparation:\n",
        "\n",
        "Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/\n",
        "datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message is spam or not based on several input features.\n",
        "\n",
        "Implementation:\n",
        "\n",
        "Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the\n",
        "dataset. You should use the default hyperparameters for each classifier.\n",
        "\n",
        "Results:\n",
        "Report the following performance metrics for each classifier:\n",
        "Accuracy\n",
        "Precision\n",
        "Recall\n",
        "F1 score\n",
        "\n",
        "Discussion:\n",
        "Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is\n",
        "the case? Are there any limitations of Naive Bayes that you observed?\n",
        "\n",
        "Conclusion:\n",
        "Summarise your findings and provide some suggestions for future work.\n",
        "\n",
        "Note: This dataset contains a binary classification problem with multiple features. The dataset is relatively small, but it can be used to demonstrate the performance of the different variants of Naive Bayes on a real-world problem.\n",
        "\n",
        "Answer:\n",
        "Sure! Below is a structured approach to your assignment, including the steps for data preparation, implementation, evaluation, and discussion of results for the Spambase dataset using Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers.\n",
        "\n",
        "Data Preparation\n",
        "\n",
        "Download the Dataset:\n",
        "\n",
        "You can download the Spambase dataset from the UCI Machine Learning Repository. The dataset is available in a .csv format or as a text file.\n",
        "The dataset contains 57 attributes, with the last column indicating whether an email is spam (1) or not (0).\n",
        "\n",
        "Load the Dataset:\n",
        "\n",
        "Use pandas to load the dataset and perform basic preprocessing, such as handling missing values if needed and separating the features and target variable."
      ],
      "metadata": {
        "id": "C91P5YUDWAtU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n"
      ],
      "metadata": {
        "id": "sEU11_BuddHy"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "# Use the direct link to the data file\n",
        "data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data', header=None)\n",
        "X = data.iloc[:, :-1]  # Features\n",
        "y = data.iloc[:, -1]   # Target variable"
      ],
      "metadata": {
        "id": "5_Rm0TfehGbw"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifiers = {\n",
        "    'BernoulliNB': BernoulliNB(),\n",
        "    'MultinomialNB': MultinomialNB(),\n",
        "    'GaussianNB': GaussianNB()\n",
        "}\n",
        "\n",
        "results = {}\n",
        "\n",
        "for name, clf in classifiers.items():\n",
        "    scores = cross_val_score(clf, X, y, cv=10, scoring='accuracy')\n",
        "    results[name] = {\n",
        "        'accuracy': scores.mean(),\n",
        "        'precision': cross_val_score(clf, X, y, cv=10, scoring='precision').mean(),\n",
        "        'recall': cross_val_score(clf, X, y, cv=10, scoring='recall').mean(),\n",
        "        'f1_score': cross_val_score(clf, X, y, cv=10, scoring='f1').mean()\n",
        "    }"
      ],
      "metadata": {
        "id": "9JGspUnjhQIn"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for classifier, metrics in results.items():\n",
        "    print(f\"{classifier} Performance:\")\n",
        "    print(f\"Accuracy: {metrics['accuracy']:.2f}\")\n",
        "    print(f\"Precision: {metrics['precision']:.2f}\")\n",
        "    print(f\"Recall: {metrics['recall']:.2f}\")\n",
        "    print(f\"F1 Score: {metrics['f1_score']:.2f}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abs802Uhfd8A",
        "outputId": "dce98612-028c-42f0-915f-b3cc659b75c2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BernoulliNB Performance:\n",
            "Accuracy: 0.88\n",
            "Precision: 0.89\n",
            "Recall: 0.82\n",
            "F1 Score: 0.85\n",
            "\n",
            "MultinomialNB Performance:\n",
            "Accuracy: 0.79\n",
            "Precision: 0.74\n",
            "Recall: 0.72\n",
            "F1 Score: 0.73\n",
            "\n",
            "GaussianNB Performance:\n",
            "Accuracy: 0.82\n",
            "Precision: 0.71\n",
            "Recall: 0.96\n",
            "F1 Score: 0.81\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Discussion\n",
        "Performance Comparison: Compare the accuracy, precision, recall, and F1 score of each classifier. Discuss which classifier performed the best and why.\n",
        "Limitations of Naive Bayes: Discuss potential limitations, such as the assumption of feature independence, sensitivity to irrelevant features, and the impact of imbalanced classes.\n",
        "\n",
        "Conclusion\n",
        "\n",
        "Summarize the findings on the performance of the different variants of Naive Bayes classifiers.\n",
        "\n",
        "Suggest future work, such as experimenting with feature selection techniques, trying out different datasets, or exploring advanced models like ensemble methods or neural networks for spam classification.\n",
        "Example of Expected Results Discussion\n",
        "\n",
        "\n",
        "Multinomial Naive Bayes often performs well in text classification tasks due to its ability to handle frequency information effectively.\n",
        "Bernoulli Naive Bayes is also strong when the presence or absence of features is key, especially in binary feature scenarios.\n",
        "Gaussian Naive Bayes may underperform if the features do not follow a Gaussian distribution.\n",
        "\n",
        "Limitations\n",
        "\n",
        "The Naive Bayes classifiers are sensitive to irrelevant features, which can lead to reduced performance.\n",
        "\n",
        "They also assume that all features are independent given the class label, which may not hold true in real-world scenarios."
      ],
      "metadata": {
        "id": "c6b4j5sBfe_o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Thank You!**"
      ],
      "metadata": {
        "id": "i_l5YnXMh97f"
      }
    }
  ]
}