{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO54Gtlf1xysqJsy/XwNoE+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/drsubirghosh2008/drsubirghosh2008/blob/main/PW_Assignment_Module_26_6_11_24_Ensemble_Techniques_%26_it's_types_Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is an ensemble technique in machine learning?\n",
        "\n",
        "Answer:\n",
        "\n",
        "In machine learning, an ensemble technique combines multiple models to improve predictive performance over a single model. By aggregating the results of several models, ensemble methods aim to create a stronger overall model that has higher accuracy and robustness, especially in cases where individual models may perform inconsistently.\n",
        "\n",
        "There are three main types of ensemble techniques:\n",
        "\n",
        "Bagging (Bootstrap Aggregating): This method involves training multiple versions of a model on different random subsets of the training data (with replacement). The most common example is Random Forests, where multiple decision trees are trained on random subsets of data, and the final output is based on the majority vote (classification) or average prediction (regression).\n",
        "\n",
        "Boosting: Boosting techniques sequentially train models, where each model tries to correct errors made by the previous one. By focusing on instances that previous models misclassified or predicted poorly, boosting reduces bias and often yields a strong final model. Examples include AdaBoost, Gradient Boosting, and XGBoost.\n",
        "\n",
        "Stacking: In stacking, multiple different models (often of varying types) are trained, and their predictions are used as inputs for a final model (meta-model) that makes the ultimate prediction. This approach allows the ensemble to leverage the strengths of diverse models, potentially achieving higher performance than any individual model.\n",
        "\n",
        "Ensemble methods are powerful in machine learning because they reduce variance, bias, or both, and tend to improve generalization on unseen data."
      ],
      "metadata": {
        "id": "30GNJZmWpHwY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. Why are ensemble techniques used in machine learning?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Ensemble techniques are used in machine learning because they enhance model performance, stability, and generalization, especially on complex datasets where a single model might struggle. Here are some key reasons why ensemble techniques are valuable:\n",
        "\n",
        "Improved Accuracy: By combining multiple models, ensembles often yield higher accuracy than any single model could achieve on its own. Different models capture different patterns or relationships in the data, so aggregating them typically provides better predictive power.\n",
        "\n",
        "Reduced Variance: Ensembles help reduce the model's variance, making predictions more consistent and less sensitive to the randomness in the training data. Techniques like bagging (e.g., Random Forest) are specifically designed to reduce variance by training multiple models on different data samples.\n",
        "\n",
        "Reduced Bias: Methods like boosting (e.g., AdaBoost, Gradient Boosting) work by sequentially training models, with each model attempting to correct the mistakes of the previous one. This approach helps reduce bias by making the ensemble more flexible and able to capture complex patterns.\n",
        "\n",
        "Enhanced Robustness: Since ensembles aggregate the predictions of multiple models, they are less likely to be impacted by the peculiarities or weaknesses of individual models. This robustness is especially helpful when dealing with noisy or imbalanced datasets.\n",
        "\n",
        "Better Generalization: Ensembles tend to generalize better to unseen data. By combining diverse models, they are less prone to overfitting the training data and more likely to provide reliable predictions on new data.\n",
        "\n",
        "Flexibility: Ensemble methods allow combining models of different types and strengths. For instance, stacking can incorporate models like decision trees, neural networks, and support vector machines in a single framework, utilizing the strengths of each.\n",
        "\n",
        "Ensemble techniques are commonly used in machine learning competitions and production systems because of their ability to deliver strong, dependable results across a wide range of problem types."
      ],
      "metadata": {
        "id": "wJMW_XEPpnth"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. What is bagging?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Bagging, short for Bootstrap Aggregating, is an ensemble technique in machine learning that aims to reduce the variance of a model by creating multiple versions of it and averaging their predictions. Bagging is particularly useful with high-variance models like decision trees, which are prone to overfitting.\n",
        "\n",
        "How Bagging Works:\n",
        "Bootstrapping the Data: Multiple subsets of the original training dataset are generated by sampling with replacement. This means each subset may contain duplicate instances and will be slightly different from the others.\n",
        "\n",
        "Training Multiple Models: A separate model is trained on each bootstrapped subset. These models are generally of the same type (e.g., all decision trees in the case of Random Forests), but because each model is trained on a different subset, they produce diverse predictions.\n",
        "\n",
        "Aggregating the Predictions: After training, the predictions from each model are combined:\n",
        "\n",
        "For classification problems, a majority vote is typically used (each model \"votes\" for a class, and the most common class is chosen).\n",
        "For regression problems, the average of all model predictions is taken.\n",
        "Benefits of Bagging\n",
        "Reduces Variance: By averaging or voting across multiple models, bagging decreases the variance, leading to a more stable and generalizable model.\n",
        "Improves Accuracy: Bagging often provides a more accurate model by averaging out errors and preventing overfitting.\n",
        "Handles High-Variance Models Well: Models prone to overfitting, like decision trees, benefit from bagging as it balances out their tendency to fit too closely to the training data.\n",
        "Example of Bagging: Random Forests\n",
        "The most common application of bagging is the Random Forest algorithm. In Random Forests, multiple decision trees are trained on bootstrapped subsets of data, and a random subset of features is used to split each node in each tree. The result is a strong, accurate ensemble model that reduces the likelihood of overfitting, which individual decision trees might experience.\n",
        "\n",
        "Bagging is a powerful technique when you want to build a model with high accuracy and low variance, especially for noisy datasets or those with complex patterns."
      ],
      "metadata": {
        "id": "9c04Ou27p0GY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. What is boosting?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Boosting is an ensemble technique in machine learning that builds a strong model by sequentially combining multiple weak learners, typically decision trees, in a way that each subsequent model focuses on the mistakes made by the previous ones. Unlike bagging, which trains models independently and aggregates their predictions, boosting creates a sequence of models that are trained iteratively to improve performance by learning from errors.\n",
        "\n",
        "How Boosting Works\n",
        "Initialize Weights or Errors: In boosting, a set of initial weights or error rates is assigned to each instance in the training data. These weights help determine how much focus the model should place on each data point.\n",
        "\n",
        "Train Models Sequentially: Boosting involves training a sequence of weak models, where each model tries to correct the errors made by the previous model. Each new model adjusts the weights (or assigns more importance) to instances that were previously misclassified or poorly predicted, aiming to reduce the error.\n",
        "\n",
        "Aggregate the Models: The predictions from all models in the sequence are combined, often with each model‚Äôs contribution weighted based on its accuracy. In this way, models that perform well are given more weight in the final prediction, creating a more accurate ensemble.\n",
        "\n",
        "Key Types of Boosting\n",
        "AdaBoost (Adaptive Boosting):\n",
        "\n",
        "AdaBoost assigns weights to each data point, increasing weights for misclassified instances and decreasing them for correctly classified ones.\n",
        "Each model is trained sequentially, focusing more on instances with higher weights.\n",
        "The final model prediction is based on a weighted vote from each model in the ensemble.\n",
        "Gradient Boosting:\n",
        "\n",
        "Gradient Boosting uses gradient descent to minimize errors in the ensemble.\n",
        "It builds each model by fitting it to the residuals (errors) of the previous model rather than the original data, effectively reducing the overall error.\n",
        "This approach is highly effective for both regression and classification tasks.\n",
        "XGBoost (Extreme Gradient Boosting):\n",
        "\n",
        "XGBoost is an optimized, faster version of Gradient Boosting that includes regularization to prevent overfitting and parallelization for faster computation.\n",
        "It‚Äôs widely used due to its high accuracy, efficiency, and scalability.\n",
        "CatBoost and LightGBM:\n",
        "\n",
        "These are more recent versions of boosting algorithms that offer additional optimizations for handling categorical data (CatBoost) and large datasets (LightGBM), while maintaining or improving the efficiency of Gradient Boosting.\n",
        "Benefits of Boosting\n",
        "Reduces Bias: Boosting sequentially corrects errors, helping to reduce bias and make the model more flexible in capturing complex patterns.\n",
        "Improves Accuracy: By focusing on errors and adjusting weights iteratively, boosting often yields high accuracy, especially with well-tuned parameters.\n",
        "Handles Complex Datasets Well: Boosting is particularly effective for complex datasets with many interactions and non-linear relationships.\n",
        "Drawbacks of Boosting\n",
        "Risk of Overfitting: Boosting can overfit if too many models are added, particularly on noisy data.\n",
        "Slower Training Time: Sequential training makes boosting slower than parallelizable methods like bagging.\n",
        "Boosting is commonly used when high accuracy is desired, and it has been successfully applied in various machine learning tasks, especially with popular implementations like AdaBoost, Gradient Boosting, and XGBoost.\n"
      ],
      "metadata": {
        "id": "6WlBZWPlp-wQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. What are the benefits of using ensemble techniques?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Ensemble techniques offer several advantages in machine learning, making them popular for building robust, accurate models. Here are the key benefits:\n",
        "\n",
        "Higher Accuracy: Ensemble techniques typically produce more accurate models by combining the strengths of multiple models. By leveraging the predictions of different models, ensembles often yield better overall performance than any single model.\n",
        "\n",
        "Reduced Overfitting: Combining several models helps mitigate overfitting, especially in high-variance models. Techniques like bagging create multiple versions of the model on varied data subsets, reducing the likelihood that the ensemble will closely fit the noise in the training data.\n",
        "\n",
        "Improved Generalization: Ensembles tend to generalize better on unseen data since they aggregate insights from multiple models. This combination reduces the risk of relying on a single model that might perform well on training data but poorly on new data.\n",
        "\n",
        "Reduced Variance and Bias: Different ensemble techniques target variance and bias:\n",
        "\n",
        "Bagging reduces variance by averaging multiple models trained on bootstrapped datasets.\n",
        "Boosting reduces bias by focusing on the errors of previous models, helping the ensemble better capture complex patterns.\n",
        "Enhanced Stability and Robustness: Ensemble methods are less sensitive to data noise, anomalies, or outliers, as individual models' weaknesses are offset by the strengths of others. This makes ensemble models more robust and stable for real-world applications.\n",
        "\n",
        "Adaptability and Flexibility: Ensembles can combine models of different types or algorithms, allowing practitioners to mix models with varying strengths. For instance, stacking enables using diverse models, like decision trees, support vector machines, and neural networks, within one ensemble to maximize predictive performance.\n",
        "\n",
        "Effectiveness on Complex Data: For datasets with complex relationships, interactions, or non-linear patterns, ensemble methods‚Äîparticularly boosting‚Äîcan build models that capture these complexities better than simpler models.\n",
        "\n",
        "Versatility Across Applications: Ensemble methods are versatile and can be applied to a wide range of machine learning tasks, from classification and regression to time series forecasting and anomaly detection. This adaptability makes them useful across industries.\n",
        "\n",
        "In summary, ensemble techniques improve the overall quality of machine learning models by combining the strengths of multiple approaches, leading to high accuracy, reduced overfitting, and more reliable predictions on unseen data. These benefits make ensembles a popular choice in both research and practical applications."
      ],
      "metadata": {
        "id": "n_UEzU3KqNOQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6. Are ensemble techniques always better than individual models?\n",
        "\n",
        "Answer:\n",
        "\n",
        "No, ensemble techniques are not always better than individual models. While ensemble methods often yield improved performance, there are cases where a single model may be preferable. Here are some situations where individual models might be more suitable than ensembles:\n",
        "\n",
        "Simple Problems: If the problem is straightforward and the dataset is relatively small or easy to model, a well-tuned individual model can perform just as well as an ensemble. In these cases, ensembles may add unnecessary complexity without significant performance gains.\n",
        "\n",
        "Overfitting Risk with Noisy Data: Boosting algorithms, in particular, can be prone to overfitting on noisy datasets. In situations where the data has a lot of noise, an individual model with regularization might generalize better than an ensemble.\n",
        "\n",
        "Increased Computational Complexity: Ensemble techniques, especially those that rely on many base models (e.g., Random Forests with hundreds of trees or large boosting ensembles), require more memory and processing power. For resource-constrained environments (like mobile devices or real-time systems), a single, simpler model can be more practical.\n",
        "\n",
        "Interpretability Needs: Ensembles are generally more complex and less interpretable than individual models. For applications where understanding model decisions is essential‚Äîsuch as healthcare, finance, or regulatory environments‚Äîusing an interpretable individual model, like a simple decision tree or logistic regression, might be preferable.\n",
        "\n",
        "Longer Training Times: Ensembles typically take longer to train due to the multiple models involved. For time-sensitive tasks or rapidly evolving data, a single model might be faster to update and deploy.\n",
        "\n",
        "Diminishing Returns in Performance: In some cases, the performance gains from using an ensemble over a well-tuned individual model might be minimal. If the additional improvement in accuracy is not substantial, the added complexity of an ensemble might not justify its use.\n",
        "\n",
        "Risk of Model Misalignment: If the individual models in an ensemble are too similar, as may happen with certain types of data, the ensemble might not significantly improve performance over a single model. The ensemble could simply replicate the limitations of the base models, offering little added benefit.\n",
        "\n",
        "In Summary\n",
        "While ensemble techniques are powerful, they are not universally better. For simpler tasks, low-resource environments, or cases requiring interpretability, a well-tuned individual model might be more suitable. However, for complex, high-stakes tasks where accuracy is critical, ensembles are often worth the additional complexity."
      ],
      "metadata": {
        "id": "aZWp9LxNqaKY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7. How is the confidence interval calculated using bootstrap?\n",
        "\n",
        "Answer:\n",
        "\n",
        "In statistics, the bootstrap method is commonly used to estimate confidence intervals, especially when the underlying distribution of the data is unknown or when the sample size is small. This technique involves resampling from the original data and is helpful in calculating confidence intervals for various statistics (e.g., mean, median, standard deviation).\n",
        "\n",
        "Steps to Calculate a Confidence Interval Using Bootstrap\n",
        "Here‚Äôs how you would calculate a confidence interval using the bootstrap method:\n",
        "\n",
        "Draw Bootstrap Samples:\n",
        "\n",
        "Randomly sample (with replacement) from the original dataset to create a new dataset (bootstrap sample) of the same size as the original.\n",
        "Repeat this process many times (typically 1,000 to 10,000 times) to generate a large number of bootstrap samples.\n",
        "Calculate the Statistic for Each Sample:\n",
        "\n",
        "For each bootstrap sample, calculate the statistic of interest (e.g., mean, median, or any other parameter you're studying).\n",
        "Create a Distribution of the Statistic:\n",
        "\n",
        "After calculating the statistic for each bootstrap sample, you will have a distribution of that statistic based on the bootstrap samples.\n",
        "Calculate the Confidence Interval:\n",
        "\n",
        "To form a (1 - Œ±) x 100% confidence interval (e.g., a 95% confidence interval with Œ± = 0.05):\n",
        "Sort the bootstrap statistics.\n",
        "Take the lower bound as the (Œ±/2) x 100th percentile of the bootstrap distribution.\n",
        "Take the upper bound as the (1 - Œ±/2) x 100th percentile of the bootstrap distribution.\n",
        "For a 95% confidence interval, this would mean taking the 2.5th percentile for the lower bound and the 97.5th percentile for the upper bound.\n",
        "Example of Bootstrapping a Confidence Interval for the Mean\n",
        "Suppose you have a dataset, and you want to estimate the 95% confidence interval for the mean:\n",
        "\n",
        "Resample the dataset (with replacement) 10,000 times.\n",
        "Calculate the mean for each of these 10,000 samples.\n",
        "Sort these 10,000 means and take the 2.5th percentile as the lower bound and the 97.5th percentile as the upper bound.\n",
        "This range represents the 95% confidence interval for the mean based on bootstrapping.\n",
        "\n",
        "Advantages of Bootstrap for Confidence Intervals\n",
        "Non-parametric: The bootstrap method does not assume a specific distribution for the data, making it versatile and applicable to many types of datasets.\n",
        "Applicable to Small Samples: It can be used effectively even when the sample size is small, where traditional methods may not provide reliable intervals.\n",
        "Key Takeaway\n",
        "The bootstrap method provides a way to empirically estimate confidence intervals by resampling data, calculating statistics, and using percentiles from the resampled distribution. This approach is flexible and robust, particularly useful when standard assumptions about the data distribution do not hold.\n"
      ],
      "metadata": {
        "id": "-PycZJ3-qlIw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8. How does bootstrap work and What are the steps involved in bootstrap?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Bootstrap is a statistical resampling technique used to estimate the distribution of a sample statistic (e.g., mean, median, or variance) by resampling with replacement from the original dataset. This technique is especially useful for estimating confidence intervals and testing hypotheses when the underlying distribution of the data is unknown or when the sample size is small.\n",
        "\n",
        "How Bootstrap Works\n",
        "Bootstrap works by creating \"bootstrap samples\" from the original dataset, allowing us to approximate the distribution of a statistic without relying on any assumptions about the data's underlying distribution.\n",
        "\n",
        "Steps Involved in Bootstrapping\n",
        "Here‚Äôs a step-by-step breakdown of how the bootstrap method works:\n",
        "\n",
        "Create Bootstrap Samples:\n",
        "\n",
        "From the original dataset of size\n",
        "ùëõ\n",
        "n, create a new sample (bootstrap sample) by randomly drawing\n",
        "ùëõ\n",
        "n observations with replacement. This means each observation can appear multiple times in a single bootstrap sample.\n",
        "Repeat this process a large number of times (e.g., 1,000 to 10,000 times) to generate multiple bootstrap samples. Each bootstrap sample should be the same size as the original dataset.\n",
        "Calculate the Statistic for Each Bootstrap Sample:\n",
        "\n",
        "For each bootstrap sample, calculate the statistic of interest (e.g., mean, median, variance). This results in a distribution of the statistic based on the bootstrap samples.\n",
        "Build the Bootstrap Distribution:\n",
        "\n",
        "After calculating the statistic for each of the bootstrap samples, you will have a distribution of that statistic. This is known as the bootstrap distribution of the statistic.\n",
        "Calculate the Confidence Interval or Other Estimates:\n",
        "\n",
        "To estimate a confidence interval for the statistic, sort the values in the bootstrap distribution.\n",
        "For a (1 - Œ±) x 100% confidence interval (e.g., 95% confidence interval):\n",
        "Take the lower bound as the (Œ±/2) x 100th percentile of the bootstrap distribution.\n",
        "Take the upper bound as the (1 - Œ±/2) x 100th percentile.\n",
        "This range represents the confidence interval for the statistic based on the bootstrap resampling.\n",
        "Example of Bootstrapping the Mean\n",
        "Suppose we have a dataset of 100 values, and we want to estimate the 95% confidence interval for the mean:\n",
        "\n",
        "Generate Bootstrap Samples: Randomly resample (with replacement) 100 values from the dataset, repeating this process 10,000 times to create 10,000 bootstrap samples.\n",
        "Calculate the Mean for Each Sample: Compute the mean of each bootstrap sample, resulting in 10,000 mean values.\n",
        "Build the Bootstrap Distribution: Sort these 10,000 means to form a distribution of sample means.\n",
        "Calculate the 95% Confidence Interval: Take the 2.5th percentile of this distribution as the lower bound and the 97.5th percentile as the upper bound.\n",
        "This range is the 95% confidence interval for the mean based on bootstrapping.\n",
        "\n",
        "Advantages of Bootstrapping\n",
        "Non-parametric: Does not rely on any specific distribution assumptions.\n",
        "Adaptable to Small Samples: Useful for small datasets where traditional methods may be unreliable.\n",
        "Flexible for Various Statistics: Can be applied to any statistic (e.g., median, proportion, correlation).\n",
        "In summary, bootstrap is a powerful technique for estimating the distribution of a statistic and its confidence intervals by generating multiple resamples from the original dataset. It‚Äôs particularly useful when the data does not meet traditional assumptions required for other methods."
      ],
      "metadata": {
        "id": "R2R4reXvqyhg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9. A researcher wants to estimate the mean height of a population of trees. They measure the height of a sample of 50 trees and obtain a mean height of 15 meters and a standard deviation of 2 meters. Use bootstrap to estimate the 95% confidence interval for the population mean height.\n",
        "\n",
        "Answer:\n",
        "\n",
        "To estimate the 95% confidence interval for the population mean height using the bootstrap method, we can follow these steps:\n",
        "\n",
        "Given Information\n",
        "Sample Mean (\n",
        "ùë•\n",
        "Àâ\n",
        "x\n",
        "Àâ\n",
        " ) = 15 meters\n",
        "Sample Standard Deviation (\n",
        "ùë†\n",
        "s) = 2 meters\n",
        "Sample Size (\n",
        "ùëõ\n",
        "n) = 50 trees\n",
        "Number of Bootstrap Resamples: Typically, we use a large number, like 10,000, to get a stable estimate.\n",
        "Steps to Calculate the Bootstrap Confidence Interval\n",
        "Generate Bootstrap Samples:\n",
        "\n",
        "Randomly resample (with replacement) from the original data 10,000 times to create 10,000 bootstrap samples.\n",
        "Each bootstrap sample should have 50 heights, simulating our original sample size.\n",
        "Calculate the Mean of Each Bootstrap Sample:\n",
        "\n",
        "For each bootstrap sample, calculate the mean height.\n",
        "This will yield a distribution of 10,000 bootstrap sample means.\n",
        "Calculate the 95% Confidence Interval:\n",
        "\n",
        "Sort the 10,000 bootstrap sample means.\n",
        "Take the 2.5th percentile as the lower bound of the confidence interval.\n",
        "Take the 97.5th percentile as the upper bound of the confidence interval.\n",
        "\n",
        "Simulation in Python\n",
        "Since we don‚Äôt have the actual tree height data, we‚Äôll generate synthetic data based on the sample mean and standard deviation provided, assuming a normal distribution for simplicity.\n",
        "\n",
        "Let‚Äôs proceed with the calculation.\n",
        "\n",
        "The 95% confidence interval for the population mean height, estimated using the bootstrap method, is approximately:\n",
        "\n",
        "Lower¬†Bound\n",
        "=\n",
        "14.03\n",
        "¬†meters\n",
        "Lower¬†Bound=14.03¬†meters\n",
        "Upper¬†Bound\n",
        "=\n",
        "15.06\n",
        "¬†meters\n",
        "Upper¬†Bound=15.06¬†meters\n",
        "Thus, based on this bootstrap estimate, we can be 95% confident that the true mean height of the population of trees lies between 14.03 and 15.06 meters."
      ],
      "metadata": {
        "id": "7QLp52aerFho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Given values\n",
        "sample_mean = 15       # Mean height of sample\n",
        "sample_std = 2         # Standard deviation of sample\n",
        "sample_size = 50       # Number of trees in the sample\n",
        "num_bootstrap_samples = 10000  # Number of bootstrap samples\n",
        "\n",
        "# Step 1: Generate a synthetic dataset using the sample mean and std dev.\n",
        "# Assume normal distribution of heights for simplicity in the synthetic data\n",
        "np.random.seed(42)  # Set seed for reproducibility\n",
        "synthetic_data = np.random.normal(loc=sample_mean, scale=sample_std, size=sample_size)\n",
        "\n",
        "# Step 2: Perform bootstrap sampling and calculate means\n",
        "bootstrap_means = [\n",
        "    np.mean(np.random.choice(synthetic_data, size=sample_size, replace=True))\n",
        "    for _ in range(num_bootstrap_samples)\n",
        "]\n",
        "\n",
        "# Step 3: Calculate the 95% confidence interval from bootstrap means\n",
        "lower_bound = np.percentile(bootstrap_means, 2.5)\n",
        "upper_bound = np.percentile(bootstrap_means, 97.5)\n",
        "lower_bound, upper_bound\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fp2TOIEFrhws",
        "outputId": "df5ef40c-2d19-4eb3-c265-cdde9fb7396c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(14.033849846852862, 15.061040878849226)"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    }
  ]
}