{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNyZnFR5xqh0Lqw0F2Y3Avz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/drsubirghosh2008/drsubirghosh2008/blob/main/PW_Assignment_Module_26_8_11_24_Boosting_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is Gradient Boosting Regression?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Gradient Boosting Regression is a machine learning technique used for regression tasks. It builds an ensemble of decision trees in a sequential manner to predict a continuous target variable. The core idea is to improve the model's performance by iteratively reducing the residual errors (differences between predicted and actual values) from the previous model.\n",
        "\n",
        "Here’s how it works:\n",
        "\n",
        "Key Concepts:\n",
        "\n",
        "Base Learner:\n",
        "\n",
        "Gradient Boosting typically uses decision trees (often shallow) as base learners.\n",
        "\n",
        "These are weak models with limited predictive power.\n",
        "\n",
        "Sequential Learning:\n",
        "\n",
        "Models are built one at a time, and each new model focuses on correcting the errors of the previous model.\n",
        "Gradient Descent:\n",
        "\n",
        "It minimizes a loss function (e.g., Mean Squared Error for regression) by fitting the next model to the negative gradient of the loss with respect to the predictions.\n",
        "This ensures that the new model moves predictions closer to the actual values.\n",
        "Ensemble Method:\n",
        "\n",
        "The final prediction is the weighted sum of all the weak learners.\n",
        "Typically, predictions are aggregated by summing up their contributions.\n",
        "Shrinkage (Learning Rate):\n",
        "\n",
        "A learning rate parameter is introduced to control the contribution of each weak learner.\n",
        "This reduces overfitting and allows for better generalization.\n",
        "Regularization:\n",
        "\n",
        "To prevent overfitting, techniques like limiting tree depth, setting minimum samples per leaf, or using subsampling are employed.\n",
        "Steps in Gradient Boosting Regression:\n",
        "Initialize the model with a constant value, often the mean of the target variable.\n",
        "Compute the residuals (errors) between predicted and actual values.\n",
        "Fit a weak learner (decision tree) to these residuals.\n",
        "Update the model by adding the weak learner’s predictions, scaled by the learning rate.\n",
        "Repeat steps 2–4 for a predefined number of iterations or until the residuals are minimized sufficiently.\n",
        "Combine all the weak learners to form the final model.\n",
        "Advantages:\n",
        "Handles both linear and nonlinear relationships effectively.\n",
        "Robust to overfitting if properly regularized.\n",
        "Offers excellent predictive performance on a wide range of datasets.\n",
        "Disadvantages:\n",
        "Computationally intensive due to its iterative nature.\n",
        "Hyperparameters like the number of trees, learning rate, and tree depth need careful tuning.\n",
        "Sensitive to noisy data and outliers.\n",
        "Libraries and Implementations:\n",
        "Scikit-learn: GradientBoostingRegressor\n",
        "XGBoost: Extreme Gradient Boosting (efficient and faster implementation).\n",
        "LightGBM: A gradient boosting framework optimized for speed and accuracy.\n",
        "CatBoost: Gradient boosting designed to handle categorical data efficiently.\n",
        "Gradient Boosting is widely used in data science competitions and real-world applications like price prediction, financial modeling, and risk analysis."
      ],
      "metadata": {
        "id": "2mK-w-OIuwdb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared.\n",
        "\n",
        "Answer:\n",
        "\n",
        "\n",
        "Here’s a step-by-step implementation of a simple Gradient Boosting Regression algorithm from scratch using Python and NumPy:"
      ],
      "metadata": {
        "id": "RMbFGWzXvbd7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import Necessary Libraries\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n"
      ],
      "metadata": {
        "id": "VQepjhoVvw2V"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Define the Gradient Boosting Regressor Class\n",
        "\n",
        "class GradientBoostingRegressor:\n",
        "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_depth = max_depth\n",
        "        self.models = []\n",
        "        self.init_value = None\n",
        "\n",
        "    def _fit_tree(self, X, residuals):\n",
        "        \"\"\"Fit a decision tree to the residuals.\"\"\"\n",
        "        from sklearn.tree import DecisionTreeRegressor\n",
        "        tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
        "        tree.fit(X, residuals)\n",
        "        return tree\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Train the model.\"\"\"\n",
        "        self.init_value = np.mean(y)  # Initialize with the mean of the target\n",
        "        predictions = np.full(y.shape, self.init_value)\n",
        "        for _ in range(self.n_estimators):\n",
        "            residuals = y - predictions\n",
        "            tree = self._fit_tree(X, residuals)\n",
        "            self.models.append(tree)\n",
        "            predictions += self.learning_rate * tree.predict(X)\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Make predictions.\"\"\"\n",
        "        predictions = np.full((X.shape[0],), self.init_value)\n",
        "        for tree in self.models:\n",
        "            predictions += self.learning_rate * tree.predict(X)\n",
        "        return predictions\n"
      ],
      "metadata": {
        "id": "UvKvsP8Nv3Cs"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Create a Simple Dataset\n",
        "\n",
        "# Generate a small synthetic dataset\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 1) * 10  # Feature matrix\n",
        "y = 3 * X[:, 0] + np.random.normal(0, 1, 100)  # Target with some noise\n",
        "\n",
        "# Split into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "VK8Rrs07v_MD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Train the Model\n",
        "\n",
        "# Train the Gradient Boosting Regressor\n",
        "gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
        "gbr.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "J2s52H_owZAb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Evaluate the Model\n",
        "# Make predictions\n",
        "y_pred = gbr.predict(X_test)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Mean Squared Error: {mse:.4f}\")\n",
        "print(f\"R-squared: {r2:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ah1CeqcOwf1j",
        "outputId": "3976b788-38a1-4c37-e6e7-e780679c9818"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 0.7803\n",
            "R-squared: 0.9909\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Visualize the Results (Optional, for better understanding):\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.scatter(X_test, y_test, color='blue', label='True Values')\n",
        "plt.scatter(X_test, y_pred, color='red', label='Predicted Values')\n",
        "plt.legend()\n",
        "plt.xlabel(\"Feature\")\n",
        "plt.ylabel(\"Target\")\n",
        "plt.title(\"Gradient Boosting Regression\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "TlCqtQfRwpTL",
        "outputId": "db3e177d-11e2-41f0-dcb3-a60db787cc0c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example Output:\n",
        "\n",
        "Mean Squared Error (MSE): Lower is better.\n",
        "\n",
        "R-squared (R²): Value close to 1 indicates good performance.\n",
        "\n",
        "This implementation uses decision trees from sklearn.tree.DecisionTreeRegressor for simplicity. It trains the model iteratively, correcting errors in each step, and evaluates its performance on the test set. The visualization compares the true values with predictions."
      ],
      "metadata": {
        "id": "ukwZ-yPEw1Jj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparameters.\n",
        "\n",
        "Answer:\n",
        "\n",
        "To optimize your model's performance by experimenting with different hyperparameters like learning rate, number of trees, and tree depth, you can use Grid Search or Random Search. Here's how to approach it:\n",
        "\n",
        "1. Define the model\n",
        "Choose a model you are working with (e.g., Gradient Boosting, Random Forest, XGBoost, etc.).\n",
        "\n",
        "2. Select the hyperparameters to tune\n",
        "Typical hyperparameters include:\n",
        "\n",
        "Learning rate: Controls the step size for updating weights (e.g., 0.001, 0.01, 0.1).\n",
        "\n",
        "Number of trees: The total number of decision trees (e.g., 50, 100, 200).\n",
        "Tree depth: Maximum depth of each tree (e.g., 3, 5, 10).\n",
        "\n",
        "3. Grid Search vs. Random Search\n",
        "\n",
        "Grid Search\n",
        "\n",
        "Exhaustively searches over a predefined grid of hyperparameter values.\n",
        "\n",
        "Random Search\n",
        "\n",
        "Samples a random combination of hyperparameters within specified ranges.\n",
        "\n",
        "4. Implementing Grid/Random Search\n",
        "Here’s an example using scikit-learn:"
      ],
      "metadata": {
        "id": "H_zwbIRkw6hb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Grid Search\n",
        "\n",
        "from sklearn.ensemble import GradientBoostingRegressor # Change to Regressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the model\n",
        "model = GradientBoostingRegressor() # Change to Regressor\n",
        "\n",
        "# Define the hyperparameters grid\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.05, 0.1],\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [3, 5, 10]\n",
        "}\n",
        "\n",
        "# Set up Grid Search\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1) # Update scoring for regression\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters\n",
        "print(\"Best parameters:\", grid_search.best_params_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TyA5n5_FyAMz",
        "outputId": "ce78a5b8-d1bd-48e9-c58a-17ddbf7db196"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters: {'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 100}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Random Search\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Define the hyperparameters distribution\n",
        "param_dist = {\n",
        "    'learning_rate': [0.001, 0.01, 0.05, 0.1],\n",
        "    'n_estimators': [50, 100, 150, 200],\n",
        "    'max_depth': [3, 4, 5, 10]\n",
        "}\n",
        "\n",
        "# Set up Random Search\n",
        "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=50, cv=5, scoring='accuracy', random_state=42, n_jobs=-1)\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters\n",
        "print(\"Best parameters:\", random_search.best_params_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxfYJaFQxx2l",
        "outputId": "a93330ea-3acc-4d8c-a95f-321b34e72f89"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters: {'n_estimators': 50, 'max_depth': 4, 'learning_rate': 0.1}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/ma/core.py:2820: RuntimeWarning: invalid value encountered in cast\n",
            "  _data = np.array(data, dtype=dtype, copy=copy,\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:1103: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the Optimized Model\n",
        "# After finding the best hyperparameters, re-train the model with those values\n",
        "# and evaluate its performance:\n",
        "best_model = grid_search.best_estimator_  # or random_search.best_estimator_\n",
        "best_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate on test data\n",
        "test_accuracy = best_model.score(X_test, y_test)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6vgf3_nybCC",
        "outputId": "de3d2521-b74d-41db-f2fb-4e9cf8479d46"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.990955065304111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use cross-validation (cv) to ensure robust performance estimation.\n",
        "\n",
        "If the dataset is large, consider using Random Search for faster results.\n",
        "Visualize results to understand the impact of hyperparameters."
      ],
      "metadata": {
        "id": "sJhcHQR8y70i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. What is a weak learner in Gradient Boosting?\n",
        "\n",
        "Answer:\n",
        "\n",
        "A weak learner in the context of Gradient Boosting is a predictive model that performs slightly better than random guessing. In more formal terms, it has a small edge over random chance in terms of predictive accuracy.\n",
        "\n",
        "Key Characteristics of a Weak Learner:\n",
        "Low Complexity: Weak learners are often simple models, such as decision stumps (trees with one split) or shallow trees.\n",
        "Slight Predictive Power: They are not strong models individually but can capture basic patterns in the data.\n",
        "High Bias: Weak learners tend to underfit the data because they are intentionally kept simple to avoid overfitting.\n",
        "Role in Gradient Boosting:\n",
        "In Gradient Boosting, multiple weak learners are sequentially trained, where each learner attempts to correct the errors (or residuals) made by the previous ones. The iterative process combines these weak learners to create a strong ensemble model with reduced bias and variance.\n",
        "\n",
        "Why Use Weak Learners?\n",
        "Control Overfitting: Weak learners focus on simple relationships in the data, helping avoid overfitting.\n",
        "Flexibility: Their simplicity makes them easy to optimize and combine during the boosting process.\n",
        "Iterative Improvement: Even weak models can add significant value when aggregated intelligently.\n",
        "The strength of Gradient Boosting lies in how these weak learners are combined into a strong predictive model through the boosting process.\n"
      ],
      "metadata": {
        "id": "K7RTKzhQzDe6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
        "\n",
        "Answer:\n",
        "\n",
        "The intuition behind the Gradient Boosting algorithm lies in sequentially improving a model by correcting its errors in an iterative fashion. Here’s a step-by-step explanation of the intuition:\n",
        "\n",
        "1. Start with a Weak Model:\n",
        "Begin with a simple predictive model (e.g., a decision tree stump).\n",
        "This weak model makes predictions but likely has errors (residuals).\n",
        "2. Focus on the Mistakes:\n",
        "Analyze where the current model is performing poorly by calculating the residuals, which are the differences between the true values and the predicted values.\n",
        "These residuals represent the gradient of the loss function with respect to the predictions.\n",
        "3. Fit a New Model to Correct Residuals:\n",
        "Train a new weak learner to predict the residuals (errors).\n",
        "Intuition: The new model aims to “adjust” the original predictions by focusing on fixing what was missed.\n",
        "4. Update Predictions:\n",
        "Combine the predictions of the current ensemble of models by adding the predictions from the new model, scaled by a learning rate (to control how much each model contributes).\n",
        "5. Repeat the Process:\n",
        "Iteratively add more weak learners, each trained to correct the residual errors from the combined predictions of all previous models.\n",
        "Over time, the ensemble becomes a strong model that captures complex patterns in the data.\n",
        "6. Stop When Errors Are Minimized:\n",
        "Continue the process until the model achieves a desired level of performance (e.g., minimized loss or convergence) or reaches a specified number of iterations.\n",
        "\n",
        "Why Does Gradient Boosting Work?\n",
        "\n",
        "Boosting = Learning from Mistakes: Each step improves the model by addressing its prior shortcomings.\n",
        "Gradient Descent Perspective: Gradient Boosting is essentially performing gradient descent in function space, minimizing the loss function by updating the model iteratively.\n",
        "Ensemble Power: Combining multiple weak learners results in a strong, accurate model.\n",
        "\n",
        "Real-World Analogy:\n",
        "\n",
        "Imagine a teacher helping a student learn math. The student starts with a basic understanding and makes mistakes. The teacher provides targeted corrections (feedback on errors). Over time, the student builds a strong understanding by systematically addressing their mistakes.\n",
        "\n",
        "This is analogous to how Gradient Boosting improves a model step-by-step!"
      ],
      "metadata": {
        "id": "tyetTX2vzt3i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Gradient Boosting builds an ensemble of weak learners sequentially, where each learner focuses on correcting the errors made by the previous ones. Here’s a breakdown of the process:\n",
        "\n",
        "Step-by-Step Explanation:\n",
        "1. Initialize the Model:\n",
        "Start with a simple initial prediction, often the mean of the target variable for regression or the log-odds for classification.\n",
        "This forms the baseline prediction for the first iteration.\n",
        "2. Compute Residuals (Errors):\n",
        "For each data point, calculate the residuals (differences) between the true target values and the predictions made by the current model.\n",
        "These residuals represent the parts of the data the model has not yet learned.\n",
        "3. Fit a Weak Learner:\n",
        "Train a new weak learner (e.g., a shallow decision tree) to predict the residuals.\n",
        "This weak learner focuses on capturing the patterns in the data that were missed by the current ensemble.\n",
        "4. Update the Ensemble:\n",
        "Add the predictions of the new weak learner to the ensemble’s predictions. The contribution of this learner is scaled by a learning rate (a small constant) to control its influence.\n",
        "Updated prediction:\n",
        "𝐹\n",
        "𝑚\n",
        "(\n",
        "𝑥\n",
        ")\n",
        "=\n",
        "𝐹\n",
        "𝑚\n",
        "−\n",
        "1\n",
        "(\n",
        "𝑥\n",
        ")\n",
        "+\n",
        "𝜂\n",
        "⋅\n",
        "ℎ\n",
        "𝑚\n",
        "(\n",
        "𝑥\n",
        ")\n",
        "F\n",
        "m\n",
        "​\n",
        " (x)=F\n",
        "m−1\n",
        "​\n",
        " (x)+η⋅h\n",
        "m\n",
        "​\n",
        " (x)\n",
        "𝐹\n",
        "𝑚\n",
        "−\n",
        "1\n",
        "(\n",
        "𝑥\n",
        ")\n",
        "F\n",
        "m−1\n",
        "​\n",
        " (x): Predictions from the previous ensemble.\n",
        "ℎ\n",
        "𝑚\n",
        "(\n",
        "𝑥\n",
        ")\n",
        "h\n",
        "m\n",
        "​\n",
        " (x): New weak learner's predictions (trained on residuals).\n",
        "𝜂\n",
        "η: Learning rate (typically a small value like 0.1).\n",
        "5. Repeat the Process:\n",
        "Iterate the process, fitting successive weak learners to the residuals of the current ensemble and updating the predictions.\n",
        "Over time, the residuals shrink as the model gets better at capturing the patterns in the data.\n",
        "6. Stop When a Stopping Criterion is Met:\n",
        "The process stops when:\n",
        "A specified number of weak learners is reached.\n",
        "The residual errors are sufficiently small (convergence).\n",
        "Additional iterations do not significantly improve the model (early stopping).\n",
        "How the Ensemble is Built:\n",
        "Each weak learner contributes a small piece to the overall prediction.\n",
        "By combining many simple learners, the ensemble captures complex patterns in the data.\n",
        "Key Components in Building the Ensemble:\n",
        "Weak Learners: Simple models like decision stumps or shallow trees.\n",
        "Sequential Addition: Learners are added one at a time, focusing on correcting previous errors.\n",
        "Learning Rate (η): Controls the contribution of each weak learner, preventing overfitting.\n",
        "Loss Function: Guides the optimization by measuring the error to minimize (e.g., Mean Squared Error for regression, Log Loss for classification).\n",
        "Intuition:\n",
        "Think of Gradient Boosting as solving a puzzle. Each weak learner (small puzzle piece) addresses a specific gap left by the previous ones. Over time, the ensemble fills in the gaps, resulting in a strong predictive model."
      ],
      "metadata": {
        "id": "cYJerUzkz-XC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Constructing the mathematical intuition of the Gradient Boosting algorithm involves understanding how the model iteratively minimizes a loss function in a functional space. Here's a step-by-step outline of the process:\n",
        "\n",
        "1. Define the Problem and Objective:\n",
        "Start with a supervised learning problem where you have:\n",
        "Features\n",
        "𝑋\n",
        "=\n",
        "{\n",
        "𝑥\n",
        "1\n",
        ",\n",
        "𝑥\n",
        "2\n",
        ",\n",
        "…\n",
        ",\n",
        "𝑥\n",
        "𝑛\n",
        "}\n",
        "X={x\n",
        "1\n",
        "​\n",
        " ,x\n",
        "2\n",
        "​\n",
        " ,…,x\n",
        "n\n",
        "​\n",
        " },\n",
        "Target values\n",
        "𝑌\n",
        "=\n",
        "{\n",
        "𝑦\n",
        "1\n",
        ",\n",
        "𝑦\n",
        "2\n",
        ",\n",
        "…\n",
        ",\n",
        "𝑦\n",
        "𝑛\n",
        "}\n",
        "Y={y\n",
        "1\n",
        "​\n",
        " ,y\n",
        "2\n",
        "​\n",
        " ,…,y\n",
        "n\n",
        "​\n",
        " },\n",
        "A model\n",
        "𝐹\n",
        "(\n",
        "𝑥\n",
        ")\n",
        "F(x) that predicts\n",
        "𝑦\n",
        "^\n",
        "=\n",
        "𝐹\n",
        "(\n",
        "𝑥\n",
        ")\n",
        "y\n",
        "^\n",
        "​\n",
        " =F(x).\n",
        "Choose a loss function\n",
        "𝐿\n",
        "(\n",
        "𝑦\n",
        ",\n",
        "𝐹\n",
        "(\n",
        "𝑥\n",
        ")\n",
        ")\n",
        "L(y,F(x)) that measures the error between predictions and true values.\n",
        "For regression: Mean Squared Error (MSE).\n",
        "For classification: Log Loss (or other suitable functions).\n",
        "2. Initialize the Model:\n",
        "Begin with a simple constant prediction,\n",
        "𝐹\n",
        "0\n",
        "(\n",
        "𝑥\n",
        ")\n",
        "F\n",
        "0\n",
        "​\n",
        " (x), to minimize the loss function:\n",
        "𝐹\n",
        "0\n",
        "(\n",
        "𝑥\n",
        ")\n",
        "=\n",
        "arg\n",
        "⁡\n",
        "min\n",
        "⁡\n",
        "𝑐\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "𝐿\n",
        "(\n",
        "𝑦\n",
        "𝑖\n",
        ",\n",
        "𝑐\n",
        ")\n",
        "F\n",
        "0\n",
        "​\n",
        " (x)=arg\n",
        "c\n",
        "min\n",
        "​\n",
        "  \n",
        "i=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " L(y\n",
        "i\n",
        "​\n",
        " ,c)\n",
        "For example:\n",
        "Regression:\n",
        "𝐹\n",
        "0\n",
        "(\n",
        "𝑥\n",
        ")\n",
        "=\n",
        "mean\n",
        "(\n",
        "𝑦\n",
        ")\n",
        "F\n",
        "0\n",
        "​\n",
        " (x)=mean(y),\n",
        "Classification:\n",
        "𝐹\n",
        "0\n",
        "(\n",
        "𝑥\n",
        ")\n",
        "=\n",
        "log-odds\n",
        "F\n",
        "0\n",
        "​\n",
        " (x)=log-odds.\n",
        "3. Iterative Process:\n",
        "At each iteration\n",
        "𝑚\n",
        "m:\n",
        "\n",
        "(a) Compute Residuals (Pseudo-Residuals):\n",
        "The residuals represent the negative gradient of the loss function with respect to the predictions\n",
        "𝐹\n",
        "𝑚\n",
        "−\n",
        "1\n",
        "(\n",
        "𝑥\n",
        ")\n",
        "F\n",
        "m−1\n",
        "​\n",
        " (x):\n",
        "𝑟\n",
        "𝑖\n",
        "(\n",
        "𝑚\n",
        ")\n",
        "=\n",
        "−\n",
        "∂\n",
        "𝐿\n",
        "(\n",
        "𝑦\n",
        "𝑖\n",
        ",\n",
        "𝐹\n",
        "𝑚\n",
        "−\n",
        "1\n",
        "(\n",
        "𝑥\n",
        "𝑖\n",
        ")\n",
        ")\n",
        "∂\n",
        "𝐹\n",
        "𝑚\n",
        "−\n",
        "1\n",
        "(\n",
        "𝑥\n",
        "𝑖\n",
        ")\n",
        "r\n",
        "i\n",
        "(m)\n",
        "​\n",
        " =−\n",
        "∂F\n",
        "m−1\n",
        "​\n",
        " (x\n",
        "i\n",
        "​\n",
        " )\n",
        "∂L(y\n",
        "i\n",
        "​\n",
        " ,F\n",
        "m−1\n",
        "​\n",
        " (x\n",
        "i\n",
        "​\n",
        " ))\n",
        "​\n",
        "\n",
        "Intuition: Residuals indicate how the model needs to adjust its predictions to minimize the loss.\n",
        "(b) Fit a Weak Learner:\n",
        "Train a weak learner\n",
        "ℎ\n",
        "𝑚\n",
        "(\n",
        "𝑥\n",
        ")\n",
        "h\n",
        "m\n",
        "​\n",
        " (x) (e.g., a shallow decision tree) to predict the residuals\n",
        "𝑟\n",
        "𝑖\n",
        "(\n",
        "𝑚\n",
        ")\n",
        "r\n",
        "i\n",
        "(m)\n",
        "​\n",
        " .\n",
        "This step aligns the weak learner with the direction of steepest descent in the loss function.\n",
        "(c) Update the Model:\n",
        "Add the new weak learner's predictions to the current model:\n",
        "𝐹\n",
        "𝑚\n",
        "(\n",
        "𝑥\n",
        ")\n",
        "=\n",
        "𝐹\n",
        "𝑚\n",
        "−\n",
        "1\n",
        "(\n",
        "𝑥\n",
        ")\n",
        "+\n",
        "𝜂\n",
        "⋅\n",
        "ℎ\n",
        "𝑚\n",
        "(\n",
        "𝑥\n",
        ")\n",
        "F\n",
        "m\n",
        "​\n",
        " (x)=F\n",
        "m−1\n",
        "​\n",
        " (x)+η⋅h\n",
        "m\n",
        "​\n",
        " (x)\n",
        "𝜂\n",
        "η: Learning rate (a small constant like 0.1 or 0.01) that controls the contribution of each weak learner.\n",
        "4. Minimize the Loss Function Locally:\n",
        "Instead of directly minimizing the global loss function, Gradient Boosting minimizes it iteratively by solving:\n",
        "ℎ\n",
        "𝑚\n",
        "(\n",
        "𝑥\n",
        ")\n",
        "=\n",
        "arg\n",
        "⁡\n",
        "min\n",
        "⁡\n",
        "ℎ\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "𝐿\n",
        "(\n",
        "𝑦\n",
        "𝑖\n",
        ",\n",
        "𝐹\n",
        "𝑚\n",
        "−\n",
        "1\n",
        "(\n",
        "𝑥\n",
        "𝑖\n",
        ")\n",
        "+\n",
        "𝜂\n",
        "⋅\n",
        "ℎ\n",
        "(\n",
        "𝑥\n",
        "𝑖\n",
        ")\n",
        ")\n",
        "h\n",
        "m\n",
        "​\n",
        " (x)=arg\n",
        "h\n",
        "min\n",
        "​\n",
        "  \n",
        "i=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " L(y\n",
        "i\n",
        "​\n",
        " ,F\n",
        "m−1\n",
        "​\n",
        " (x\n",
        "i\n",
        "​\n",
        " )+η⋅h(x\n",
        "i\n",
        "​\n",
        " ))\n",
        "This ensures that each weak learner incrementally improves the model.\n",
        "5. Repeat Until Stopping Criterion:\n",
        "Continue the process until:\n",
        "A fixed number of iterations\n",
        "𝑀\n",
        "M is reached.\n",
        "The residuals or overall loss converge.\n",
        "Additional learners do not significantly improve the model.\n",
        "Mathematical Optimization Perspective:\n",
        "Gradient Boosting can be viewed as gradient descent in functional space:\n",
        "Instead of optimizing parameters, it optimizes functions (weak learners).\n",
        "The gradient of the loss function is used to guide each step in building the model.\n",
        "6. Final Prediction:\n",
        "After\n",
        "𝑀\n",
        "M iterations, the final model is:\n",
        "𝐹\n",
        "𝑀\n",
        "(\n",
        "𝑥\n",
        ")\n",
        "=\n",
        "𝐹\n",
        "0\n",
        "(\n",
        "𝑥\n",
        ")\n",
        "+\n",
        "𝜂\n",
        "∑\n",
        "𝑚\n",
        "=\n",
        "1\n",
        "𝑀\n",
        "ℎ\n",
        "𝑚\n",
        "(\n",
        "𝑥\n",
        ")\n",
        "F\n",
        "M\n",
        "​\n",
        " (x)=F\n",
        "0\n",
        "​\n",
        " (x)+η\n",
        "m=1\n",
        "∑\n",
        "M\n",
        "​\n",
        " h\n",
        "m\n",
        "​\n",
        " (x)\n",
        "Key Intuitions Behind the Math:\n",
        "Gradient Descent Analogy: At each step, the model moves in the direction of the gradient to reduce the loss.\n",
        "Residual Correction: Each weak learner is trained to address the errors (residuals) of the current model.\n",
        "Function Approximation: The ensemble of weak learners collectively approximates the true function\n",
        "𝐹\n",
        "(\n",
        "𝑥\n",
        ")\n",
        "F(x) that maps features to targets.\n",
        "By iteratively minimizing the loss in this manner, Gradient Boosting constructs a strong predictive model from a series of weak learners."
      ],
      "metadata": {
        "id": "VP2VOlbo0O9x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Thank You!**"
      ],
      "metadata": {
        "id": "yv5OHhAf0lgZ"
      }
    }
  ]
}